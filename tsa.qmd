---
title: "DSM-3001 Time Series Analysis"
author: "Mohammad Wasiq"
format: html
---

`format: docx`

This course is taught by [**Prof. Dr. Athar Ali Khan**](https://www.amu.ac.in/faculty/statistics-and-operations-research/athar-ali-khan)

# Syllabus

## Unit --- I
**Characteristics of Time Series :** The nature of time series data, time series statistical models,  measures of dependence, stationary time series, estimation of correlation, vector-valued and  multidimensional series. time series regression and exploratory data analysis: classical regression in the time series context, exploratory data analysis, smoothing in the time series context.

## Unit --- II
**ARIMA Models :** Autoregressive moving average models, difference equations, autocorrelation and partial autocorrelation, forecasting, estimation, integrated models for nonstationary data, building ARIMA Models, regression with autocorrelated errors, multiplicative seasonal ARIMA Models.

## Unit --- III
**Spectral Analysis and Filtering :** cyclical behavior and periodicity, the spectral density, 
periodogram and discrete fourier transform, nonparametric spectral estimation, parametric 
spectral estimation, multiple series and cross-spectra, linear filters, GARCH Models, Long 
Memory ARMA and Fractional Differencing, Unit Root Testing.

### Unit --- IV
**State Space Models :** Linear Gaussian Model, filtering, smoothing, and forecasting, maximum 
likelihood estimation, missing data modifications, structural models: signal extraction and 
forecasting, state-space models with correlated errors, bootstrapping state space models.

### Books
1. **Time Series Analysis With Applications in R**

**Second Edition**

**Jonathan D. Cryer , Kung-Sik Chan**

2. **Time Series Analysis for the State-Space Model with R|Stan**

**Junichiro Hagiwara**

**2021**

# UNIT --- I
**Characteristics of Time Series :** The nature of time series data, time series statistical models,  measures of dependence, stationary time series, estimation of correlation, vector-valued and multidimensional series. time series regression and exploratory data analysis: classical regression in the time series context, exploratory data analysis, smoothing in the time series context.

# Introduction to Time Series 

Data obtained from observations collected sequentially over time are extremely common.In business, we observe weekly interest rates, daily closing stock prices, monthly price indices, yearly sales figures, and so forth. In meteorology, we observe daily high and low temperatures, annual precipitation and drought indices, and hourly wind speeds. In agriculture, we record annual figures for crop and livestock production, soil erosion, and export sales. In the biological sciences, we observe the electrical activity of the heart at millisecond intervals. In ecology, we record the abundance of an animal species.

The purpose of time series analysis is generally twofold: to understand or model the stochastic mechanism that gives rise to an observed series and to predict or forecast the future values of a series based on the history of that series and, possibly, other related series or factors.

## Examples of Time Series

### Annual Rainfall in Los Angeles
Exhibit 1.1 displays a time series plot of the annual rainfall amounts recorded in Los Angeles, California, over more than 100 years. The plot shows considerable variation in rainfall amount over the years—some years are low, some high, and many are in-between in value. The year 1883 was an exceptionally wet year for Los Angeles, While 1983 was quite dry. For analysis and modeling purposes we are interested in whether or not consecutive years are related in some way. If so, we might be able to use one year’s rainfall value to help forecast next year’s rainfall amount. One graphical way to investigate that question is to pair up consecutive rainfall values and plot the resulting scatterplot of pairs.

### Time Series Plot of Los Angeles Annual Rainfall
```{r rfla, wrnings=FALSE}
library(TSA)
data(larain)
head(larain)
plot(larain, ylab='Inches', xlab='Year' ,type='o')
```

Above figure shows scatterplot for rainfall. For example, the point plotted near the lower right-hand corner shows that the year of extremely high rainfall, 40 inches in 1883, was followed by a middle of the road amount (about 12 inches) in 1884. The point near the top of the display shows that the 40 inch year was preceded by a much more typical year of about 15 inches.

### Scatterplot of LA Rainfall versus Last Year’s LA Rainfall
```{r scatter,warnigs=FALSE}
plot(y=larain, x=zlag(larain), ylab='Inches',
xlab='Previous Year Inches')
```

The main impression that we obtain from this plot is that there is little if any information about this year’s rainfall amount from last year’s amount. The plot shows no “trends” and no general tendencies. There is little correlation between last year’s rainfall amount and this year’s amount. From a modeling or forecasting point of view, this is not a very interesting time series.

## An Industrial Chemical Process
As a second example, we consider a time series from an industrial chemical process. The variable measured here is a color property from consecutive batches in the process. Exhibit 1.3 shows a time series plot of these color values. Here values that are neighbors in time tend to be similar in size. It seems that neighbors are related to one another.

### Time Series Plot of Color Property from a Chemical Process
```{r colorp,warnigs=FALSE}
data(color)
plot(color, ylab='Color Property', xlab='Batch', type='o')
```

This can be seen better by constructing the scatterplot of neighboring pairs as we did with the first example.

Exhibit 1.4 displays the scatterplot of the neighboring pairs of color values. We see a slight upward trend in this plot—low values trend to be followed in the next batch by low values, middle-sized values tend to be followed by middle-sized values, and high values tend to be followed by high values. The trend is apparent but is not terribly strong. For example, the correlation in this scatterplot is about 0.6.

### Scatterplot of Color Value versus Previous Color Value
```{r scattercolor}
plot(y=color, x=zlag(color), ylab='Color Property',
xlab='Previous Batch Color Property')
```

## Annual Abundance of Canadian Hare
Our third example concerns the annual abundance of Canadian hare. Exhibit 1.5 gives the time series plot of this abundance over about 30 years. Neighboring values here are very closely related. Large changes in abundance do not occur from one year to the next.
This neighboring correlation is seen clearly in Exhibit 1.6 where we have plotted abundance versus the previous year’s abundance. As in the previous example, we see an upward trend in the plot—low values tend to be followed by low values in the next year, middle-sized values by middle-sized values, and high values by high values.

### Abundance of Canadian Hare
```{r abundace}
data(hare)
plot(hare,ylab='Abundance',xlab='Year',type='o')
```

### Hare Abundance versus Previous Year’s Hare Abundance
```{r abundancescatter}
plot(y=hare, x=zlag(hare), ylab='Abundance',
xlab='Previous Year Abundance')
```

## Monthly Average Temperatures in Dubuque, Iowa
The average monthly temperatures (in degrees Fahrenheit) over a number of years recorded in Dubuque, Iowa, are shown in Exhibit 1.7.
This time series displays a very regular pattern called seasonality. Seasonality for monthly values occurs when observations twelve months apart are related in some manner or another. All Januarys and Februarys are quite cold but they are similar in value and different from the temperatures of the warmer months of June, July, and August, for example. There is still variation among the January values and variation among the June values. Models for such series must accommodate this variation while preserving the similarities. Here the reason for the seasonality is well understood—the Northern
Hemisphere’s changing inclination toward the sun.

### Average Monthly Temperatures, Dubuque, Iowa
```{r temprature}
data(tempdub)
plot(tempdub, ylab='Temperature', type='o')
```

## Monthly Oil Filter Sales
Our last example for this chapter concerns the monthly sales to dealers of a specialty oil filter for construction equipment manufactured by John Deere. When these data were first presented to one of the authors, the manager said, “There is no reason to believe
that these sales are seasonal.” Seasonality would be present if January values tended to be related to other January values, February values tended to be related to other February values, and so forth. The time series plot shown in Exhibit 1.8 is not designed to display
seasonality especially well. Exhibit 1.9 gives the same plot but amended to use meaningful plotting symbols. In this plot, all January values are plotted with the character J, all Februarys with F, all Marches with M, and so forth.† With these plotting symbols, it is much easier to see that sales for the winter months of January and February all tend to be high, while sales in September, October, November, and December are gener-ally quite low. The seasonality in the data is much easier to see from this modified time series plot.

### Monthly Oil Filter Sales
```{r monthlysaels}
data(oilfilters)
plot(oilfilters, type='o', ylab='Sales')
```

## Monthly Oil Filter Sales with Special Plotting Symbols
```{r scattersales}
plot(oilfilters, type='l', ylab='Sales')
points(y=oilfilters, x=time(oilfilters),
pch=as.vector(season(oilfilters)))
```

# Fundamental Concepts of Time Series
We introduce the concepts of stochastic processes, mean and covariance functions, stationary processes, and autocorrelation functions.

##  Time Series and Stochastic Processes
The sequence of random variables ${Y_t : t = 0, \pm 1, \pm 2, \pm 3,\cdots}$ is called a stochastic process and serves as a model for an observed time series. It is known that the complete probabilistic structure of such a process is determined by the set of distributions of all finite collections of the $Y's$. 
<br> Fortunately, we will not have to deal explicitly with these multivariate distributions. Much of the information in these joint distributions can be described in terms of means, variances, and covariances. Consequently, we concentrate our efforts on these first and second moments. (If the joint distributions of the $Y's$ are multivariate normal distributions, then the first and second moments completely determine all the joint distributions.)

### Means, Variances, and Covariances
For a stochastic process ${Y_t: t = 0, \pm1, \pm2, \pm3 \ldots}$, the __mean function__ is defined by
$$\mu_t= E(Y_t)  \quad for\; t = 0, \pm1 \pm2 \ldots$$
That is $\mu_t$  is just the expected value of the process at time $t$. In general, $\mu_t$  can be different at each time point $t$.

The **autocovariance** function, $\gamma_{t,s}$, is defined as 
$$\gamma_{t ,s}= Cov (Y_t, Y_s) \quad for\quad t, s = 0, \pm 1,\pm 2,\ldots$$.
$$where\quad Cov(Y_t, Y_s) = E((Y_t − \mu_t)(Y_s − \mu_s)) =E(Y_t\,Y_s) − \mu_t \mu_s$$

The **autocorrelation** function, $\rho_{t,s}$, is given by
$$\rho_{t ,s}=Corr( Y_t, Y_s),\quad for\quad t, s = 0, \pm1\pm2,\ldots$$

where
$$corr(y_t,y_s)=\frac{cov(y_t,y_s)}{\sqrt{var(y_t)\,\,var(y_s)}}=\frac{\gamma_{t,s}}{\sqrt{\gamma_{t,t} \,\,\gamma_{s,s}}}$$

The following **important properties** follow from known results and our definitions:
$$\bullet \quad \gamma_{t, t} = Var( Y_t) \Rightarrow \rho_{t,t}=1$$
$$\bullet \quad \gamma_{t,s}= \gamma_{s, t} \Rightarrow \rho_{t,s}=\rho_{s,t}$$
$$\bullet \quad \gamma_{t s} \le \gamma_{t,t} \gamma_{s,s} \Rightarrow \rho_{t,s} \le 1$$
Values of $\rho_{t,s}$ near $\pm1$ indicate strong (linear) dependence, whereas values near zero indicate weak (linear) dependence. If $ρ_{t,s} = 0$, we say that $Y_t$  and $Y_s$ are uncorrelated.

To investigate the covariance properties of various time series models, the following result will be used repeatedly :
<br> If $c_1, c_2,\ldots c_m$ and $d_1, d_2,\dots, d_n$ are constants and $t_1,t_2,\ldots, t_m$ and $s_1, s_2,\ldots, s_n$ are time points, then 
$$cov[\sum_{i=1}^m c_iY_{t_i},\sum_{j=1}^n d_jY_{s_j}]=\sum_{i=1}^m \sum_{j=1}^n c_id_jcov(Y_{t_i},Y_{s_j})$$

## The Random Walk
Let $e_1, e_2,\ldots$ be a sequence of independent, identically distributed random variables each with zero mean and variance $\sigma_e^2$. The observed time series, ${Y_t: t = 1, 2,\dots}$, is constructed as follows:
$$Y_1 = e_1$$
$$Y_2 = e_1 + e_2$$
$$\vdots$$
$$Y_t= e_1 + e_2 + \dots + e_t$$
Alternatively, we can write
$$Y_t= Y_{t-1} + e_t$$

with initial condition $Y_1 = e_1$

### Means, Variances, and Covariances
**Mean**
$$\mu_t = E(Y_t) = E(e_1 + e_2 + \dots + e_t)$$
$$\mu_t = E(e_1) + E(e_2) + \dots + E(e_t)$$
$$\mu_t = 0 + 0 + \dots + 0$$
$$\mu_t = 0$$
so that $\mu_t = 0$ for all $t$.

We also have **variance** 
$$Var(Y_t) = Var(e_1 + e_2 + \dots + e_t)$$
$$Var(Y_t) = Var(e_1) + Var(e_2) + \dots + Var(e_t)$$
$$Var(Y_t) = \sigma_e^2 + \sigma_e^2 + \dots + \sigma_e^2 $$
$$Var(Y_t) = t \, \sigma_e^2$$

Notice that the process variance increases with linearly time $t$.

To investigate of a **covariance** function of a random walk.
<br> Suppose we have $1 \le t \le s$, then we have,
$$Y_{t,s} = Cov(Y_t, Y_s) = Cov(e_1 + e_2 + \dots + e_t, \quad e_1 + e_2 + \dots + e_s)$$
$$cov[\sum_{i=1}^m c_iY_{t_i},\sum_{j=1}^n d_jY_{s_j}]=\sum_{i=1}^m \sum_{j=1}^n c_id_j cov(Y_{t_i}, Y_{s_j})$$
we have,
$$cov(Y_t,Y_s) = \sum_{i=1}^m \sum_{j=1}^n cov(e_i, e_j)$$
However these covariances are $0$ unless $i=j$ in which case they are equal variance of $e_i = \sigma_e^2$.

There are exactly $t$ of these so that $\gamma_{t,s} = t\,\sigma_e^2$.

Since $\gamma_{t,s} = \gamma_{s,t}$, this specifies the **autocovariance** function for all time points $t$ and $s$ and we can write
$$\gamma_{t,s} = t\,\sigma_e^2 \quad for \quad 1 \le t \le s$$
The autocorrelation function for the random walk is now easily obtained as
$$\rho_{t,s}=\frac{\gamma_{t,s}}{\sqrt{\gamma_{t,t} \,\,\gamma_{s,s}}} = \sqrt{\frac{t}{s}} \quad for \quad 1 \le t \le s$$
The following numerical values help us understand the behavior of the random walk.
$$\rho_{1,2}=\sqrt{\frac{1}{2}}=0.707 \quad \quad \rho_{8,9}=\sqrt{\frac{8}{9}}=0.943$$
$$\rho_{24,25}=\sqrt{\frac{24}{25}}=0.980 \quad \quad \rho_{1,25}=\sqrt{\frac{1}{25}}=0.200$$
The values of $Y$ at neighboring time points are more and more strongly and positively correlated as time goes by. On the other hand, the values of $Y$ at distance time points(1,25) are less and less correlated.

```{r, warning=FALSE, message=FALSE}
require(TSA)
data("rwalk")
head(rwalk)
plot(rwalk, type="o", ylab = "Random Walk")
```

## A Moving Average
As a second example, suppose that $Y_t$ is constructed as
$$Y_t = \frac{e_t + e_{t-1}}{2}$$
where the $e's$ are assumed to be independent and identically distributed with mean $zero$ and variance $\sigma_e^2$. Here **mean**
$$\mu_t = E(Y_t) = E\Big(\frac{e_t + e_{t-1}}{2} \Big) = \frac{E(e_t) + E(e_{t-1})}{2}=0$$
and **variance**
$$Var(Y_t) = Var\Big(\frac{e_t + e_{t-1}}{2} \Big) = \frac{1}{4}Var(e_t) + Var(e_{t-1})$$
$$Var(Y_t)= \frac{1}{4}2\,\sigma_e^2= 0.5\,\sigma_e^2$$
Also **Covariance**,
$$Cov(Y_t, Y_{t-1}) = Cov\Big(\frac{e_t + e_{t-1}}{2},\frac{e_{t-1} + e_{t-2}}{2} \Big)$$
$$ = \frac{Cov(e_t, e_{t-1}) + Cov(e_t, e_{t-2}) + Cov(e_{t-1}, e_{t-1}) + Cov(e_{t-1}, e_{t-2})}{4}$$
$$ = \frac{Cov(e_{t-1}, e_{t-1})}{4} \quad \text{(as all the other covariances are zero)}$$
$$Cov(Y_t, Y_{t-1}) = 0.25\, \sigma_e^2 \quad for \, all \,\, t$$

Furthermore
$$Cov(Y_t, Y_{t-2}) = Cov\Big(\frac{e_t + e_{t-1}}{2},\frac{e_{t-2} + e_{t-3}}{2} \Big)$$
$$Cov(Y_t, Y_{t-2})= 0 \quad \text{since the e's are independent}$$
Similarly, $Cov(Y_t,Y_{t−k})= 0$ for $k>1$, so we may write

$$
\gamma_{t,s} = \begin{cases} 
  0.5  \,  \sigma_e^2 \, ,  & \text{for |t - s| = 0} \\
  0.25 \,  \sigma_e^2 \, ,  & \text{for |t - s| = 1} \\
  0    \,               ,   & \text{for |t - s| > 1}
\end{cases}
$$

For the autocorrelation function, we have
$$
\rho_{t,s} = \begin{cases} 
  1   \,  , & \text{for |t - s| = 0} \\
  0.5 \,  , & \text{for |t - s| = 1} \\
  0   \,  , & \text{for |t - s| > 1}
\end{cases}
$$

$$\rho_{t,t-1}=\frac{\gamma_{t,s}}{\sqrt{\gamma_{t,t} \,\,\gamma_{s,s}}}= \frac{0.25\sigma_e^2}{\sqrt{0.5\sigma_e^2. 0.5\sigma_e^2}}= \frac{1}{2}=0.5$$
Notice that $\rho_{2,1}= \rho_{3,2}= \rho_{4,3}= \rho_{9,8}= 0.5$. Values of $Y$ precisely one time unit apart have exactly the same correlation no matter where they occur in time. 
<br> Furthermore, $\rho_{3,1}= \rho_{4,2}= \rho_{t,t−2}$ and, more generally, $\rho_{t,t−k}$ is the same for all values of $t$. This leads us to the important concept of stationarity.

## Stationarity
The basic idea of stationarity is that the probability laws that govern the behavior of the process do not change over time. In a sense, the process is in statistical equilibrium. Specifically, a process $\{Y_t\}$ is said to be **strictly stationary** if the joint distribution of $Y_{t_1}, Y_{t_2},...,Y_{t_n}$ is the same as the joint distribution of $Y_{t_1-k}, Y_{t_2-k},...,Y_{t_n-k}$ for all choices of time points $t_1,t_2,...,t_n$ and all choices of time lag $k$.

Thus, when $n=1$ the (univariate) distribution of $Y_t$ is the same as that of $Y_{t − k}$ for all $t$ and $k;$ in other words, the $Y's$ are (marginally) identically distributed. It then follows that $E(Y_t)=E(Y_{t−k})$ for all $t$ and $k$ so that the mean function is constant for all time.

Additionally, $Var(Y_t)=Var(Y_{t−k})$ for all $t$ and $k$ so that the variance is also constant over time.

Setting $n=2$ in the stationarity definition we see that the bivariate distribution of $Y_t$ and $Y_s$ must be the same as that of $Y_{t−k}$ and $Y_{s−k}$ from which it follows that $Cov(Y_t, Y_s)= Cov(Y_{t−k}, Y_{s−k})$ for all $t$, $s$, and $k$. Putting $k=s$ and then $k=t$, we obtain
$$\gamma_{t,s}= Cov(Y_{t−s}, Y_0)$$
$$= Cov(Y_0, Y_{s−t})$$
$$= Cov(Y_0, Y_{|s−t|})$$
$$\gamma_{t,s}= \gamma_{0,|s−t|}$$
That is, the covariance between $Y_t$ and $Y_s$ depends on time only through the time difference $|t−s|$ and not otherwise on the actual times $t$ and $s$. Thus, for a stationary process, we can simplify our notation and write
$$\gamma_k= Cov(Y_t, Y_{t−s})$$
and
$$\rho_k= Corr(Y_t, Y_{t−s})$$
Note also that
$$\rho_k= \frac{\gamma_k}{\gamma_0}$$
The general properties now become
$$\bullet \quad  \gamma_k= Cov(Y_t, Y_t)= Var(Y_t) \Rightarrow \rho_0= 0$$
$$\bullet \quad \gamma_k= \gamma_{-k} \Rightarrow \rho_k= \rho_{-k}$$
$$\bullet \quad |\gamma_k| \le \gamma_0 \Rightarrow |\rho_k| \le 1$$
If a process is strictly stationary and has finite variance, then the covariance function must depend only on the time lag $k$.

### Weakly Stationarity
A stochastic process $\{Y_t\}$ is said to be **weakly** (or **second-order**)
**stationary** if
1. The mean function is constant over time, and
2. $\gamma_{t,t-k}= \gamma_{0,k}$ for all time $t$ and lag $k$

### White Noise
A very important example of a stationary process is the so-called **white noise** process, which is defined as a sequence of independent, identically distributed random variables $\{e_t\}$. Its importance stems not from the fact that it is an interesting model itself but from the fact that many useful processes can be constructed from white noise. The fact that $\{e_t\}$ is strictly stationary is easy to see since
as required. Also, μt
$$Pr(e_{t_1}\le x_1, e_{t_2}\le x_2,...,e_{t_n}\le x_n)$$
$$= Pr(e_{t_1}\le x_1) Pr(e_{t_2}\le x_2)...Pr(e_{t_n}\le x_n) \quad \text{(by independence)}$$
$$= Pr(e_{t_1-k}\le x_1) Pr(e_{t_2-k}\le x_2)...Pr(e_{t_n-k}\le x_n) \quad \text{(identical distribution)}$$
$$= Pr(e_{t_1-k}\le x_1, e_{t_2-k}\le x_2,...,e_{t_n-k}\le x_n) \quad \text{(by independence)}$$
as required. Also, $\mu_t=E(e_t)$ is constant and
$$
\gamma_k = \begin{cases} 
  Var(e_i)   \, , & \text{for k = 0} \\
  0   \,        , & \text{for k} \neq 0
\end{cases}
$$
Alternatively, we can write
as required. Also, $\mu_t=E(e_t)$ is constant and
$$
\rho_k = \begin{cases} 
  1   \,  , & \text{for k = 0} \\
  0   \,  , & \text{for k} \neq 0
\end{cases}
$$

#### Moving Average of White Noise
The moving average example where $Y_t=(e_t + e_{t−1})/2$, is another example of a stationary process constructed from white noise. In our new notation, we
have for the moving average process that
$$
\rho_k = \begin{cases} 
  1   \,  , & \text{for k = 0} \\
  0.5 \,  , &  \text{for |k|=1} \\
  0   \,  , & \text{for |k|} \ge 2
\end{cases}
$$

### Random Cosine Wave
As a somewhat different example, consider the process defined as follows :
$$Y_t= cos\Bigg[2\pi \Big(\frac{t}{12}+ \Phi \Big) \Bigg] \quad t=\pm1, \pm2,...$$
where $\Phi \sim U(0,1)$
$$\mu_t= E(Y_t)= E\Bigg\{ cos\Bigg[2\pi \Big(\frac{t}{12}+ \Phi \Big) \Bigg]\Bigg\}$$
$$\mu_t= E(Y_t)= \int_{0}^1 cos\Bigg[2\pi \Big(\frac{t}{12}+ \phi \Big) \Bigg] d\phi$$
$$= \frac{1}{2\pi} sin\Bigg[2\pi \Big(\frac{t}{12}+ \phi \Big) \Bigg]_{\phi=0}^{\phi=1}$$
$$= \frac{1}{2\pi} \Bigg[sin\Big(2\pi\frac{t}{12}+ 2\pi \Big) - sin\Big(2\pi\frac{t}{12}\Big) \Bigg]$$
$$\mu_t= E(Y_t)= 0 \quad \text{for all t} \quad \because sin(2\pi)=0$$
Also
$$\gamma_{t,s}= E\Bigg\{ cos\Bigg[2\pi \Big(\frac{t}{12}+ \Phi \Big) \Bigg]cos\Bigg[2\pi \Big(\frac{s}{12}+ \Phi \Big) \Bigg]\Bigg\}$$
$$ = \int_{0}^1 cos\Bigg[2\pi \Big(\frac{t}{12}+ \phi \Big) \Bigg]cos\Bigg[2\pi \Big(\frac{s}{12}+ \phi \Big) \Bigg] d\phi$$
$$ = \frac{1}{2}\int_{0}^1 \Bigg\{cos\Bigg[2\pi \Big(\frac{t-s}{12}\Big) \Bigg]cos\Bigg[2\pi \Big(\frac{t+s}{12}+ 2\phi \Big) \Bigg]\Bigg\} d\phi$$
$$ = \frac{1}{2} \Bigg\{cos\Bigg[2\pi \Big(\frac{t-s}{12}\Big) \Bigg] + \frac{1}{4\pi}sin\Bigg[2\pi \Big(\frac{t+s}{12}+ 2\phi \Big) \Bigg]_{\phi=0}^{\phi=1}\Bigg\}$$
$$\gamma_{t,s}= \frac{1}{2}cos\Bigg[2\pi \Bigg(\frac{|t-s|}{12}\Bigg) \Bigg]$$
So the process is stationary with autocorrelation function
$$\rho_k= cos\Big(2\pi\frac{k}{12}\Big) \quad for\,\, k=0,\pm1,\pm2,...$$

## Different Series
Suppose that instead of analyzing $\{Y_t\}$ directly, we consider the differences of successive $Y$ values denoted by :
$$\nabla Y_t= Y_t-Y_{t-1}= e_t$$
So, that different t-series $\{\nabla Y_t\}$ is stationary.

# Trends

## Regression Methods for Estimation of Trends
The classical statistical method of regression analysis may be readily used to estimate the parameters of common nonconstant mean trend models. We shall consider the most useful ones : **linear, quadratic, seasonal means** and **cosine trends**.

### Linear and Quadratic Trends in Time
Consider the deterministic time trend expressed as
$$\mu_t= \beta_0 + \beta_1 t$$
Thus,  least squares method suggest that we will minimize
$$Q(\beta_0, \beta_1)= \sum_{t=1}^n[Y_t-(\beta_0+\beta_1 t)]^2$$
The solution may be obtain
$$\hat{\beta_1}= \frac{\sum_{t=1}^n (Y_t-\bar{Y_t})(t-\bar{t})}{\sum_{t=1}^n (t-\bar{t})^2}$$
and
$$\hat{\beta_0} = \bar{Y}-\hat{\beta_1} \bar{t}$$
where $t=\frac{n(n+1)}{2n}= \frac{(n+1)}{n}$, $t=1,2,...,n$

**Example :** Consider the random walk process that was shown in Exhibit 2.1. Suppose we (mistakenly) treat this as a linear time trend and estimate the slope and intercept by least-squares regression. Using statistical software we obtain Exhibit 3.1.
```{r, message=FALSE, warning=FALSE}
library(TSA)
data(rwalk)

model1<- lm(rwalk~ time(rwalk))
summary(model1)

plot(rwalk, type='o', ylab='y')
abline(model1) # add the fitted least squares line from model1
```

### Cyclical or Seasonal Trends
Consider now modeling and estimating seasonal trends, such as for the average monthly temperature data in Exhibit 1.7. Here we assume that the observed series can be represented as
$$Y_t= \mu_t + X_t$$
where $E(X_t)=0$ for all $t$.

The most general assumption for $\mu_t$ with monthly seasonal data is that there are $12$ constants (parameters), $\beta_1, \beta_2,...,\beta_12$, giving the expected average temperature for each of the $12$ months. We may write
$$
\mu_t= \begin{cases}
\beta_1, \quad \text{for t=1,13,25,...}\\
\beta_2, \quad \text{for t= 2,14,26,..}\\
\vdots\\
\beta_12, \quad \text{for t=12,24,36,...}
\end{cases}
$$
This is sometimes called a **seasonal means** model.

```{r}
data(tempdub)
month<- season(tempdub) # period added to improve table display
model2<- lm(tempdub ~ month-1) # -1 removes the intercept term 
summary(model2)
```

```{r}
model3<- lm(tempdub~ month) # January is dropped automatically
summary(model3)
```

### Cosine Trends
The seasonal means model for monthly data consists of 12 independent parameters and does not take the shape of the seasonal trend into account at all. For example, the fact that the March and April means are quite similar (and different from the June and July means) is not reflected in the model. In some cases, seasonal trends can be modeled economically with cosine curves that incorporate the smooth change expected from one time period to the next while still preserving the seasonality.

Consider the cosine curve with equation
$$\mu_t= \beta cos(2\pi ft + \Phi)$$
We call $\beta(>0)$ *the amplitude*, $f$ *the frequency*, and $\Phi$ the phase of the curve. As $t$ varies, the curve oscillates between a maximum of $\beta$ and a minimum of $−\beta$. Since the curve repeats itself exactly every $1/f$ time units, $1/f$ is called the period of the cosine wave. $\Phi$ serves to set the arbitrary origin on the time axis. For monthly data with time indexed as $1,2,...,$ the most important frequency is $f=1/12$, because such a cosine wave will repeat itself every $12$ months. We say that the period is $12$.

Equation (3.3.4) is inconvenient for estimation because the parameters $\beta$ and $\Phi$ do not enter the expression linearly. Fortunately, a trigonometric identity is available that reparameterizes (3.3.4) more conveniently, namely
$$\beta \,cos(2\pi ft + \Phi)= \beta_1 \,cos(2\pi ft) + \beta_2 \,sin(2\pi ft)$$
where
$$\beta= \sqrt{\beta_1^2 + \beta_2^2}, \quad \Phi= atan(-\beta_2/\beta_1)$$
and conversely,
$$\beta_1= \beta\, cos(\Phi), \quad \beta_2= \beta\, sin(\Phi)$$
To estimate the parameters $\beta_1$ and $\beta_2$ with regression techniques, we simply use $cos(2\pi ft)$ and $sin(2\pi ft)$ as regressors or predictor variables.

The simplest such model for the trend would be expressed as
$$\mu_t= \beta_0 + \beta_1 \,cos(2\pi ft) + \beta_2 \,sin(2\pi ft)$$
Here the constant term, $\beta_0$, can be meaningfully thought of as a cosine with frequency zero.
```{r}
har<- harmonic(tempdub, 1)
model4<- lm(tempdub~ har)
summary(model4)
```
```{r}
# win.graph(width=4.875, height=2.5,pointsize=8)
plot(ts(fitted(model4), freq=12, start=c(1964,1)), 
ylab= 'Temperature', type='l', ylim= range(c(fitted(model4), tempdub))) ; points(tempdub)
# ylim ensures that the y axis range fits the raw data and thefitted values
```

##  Residual Analysis
As we have already noted, the unobserved stochastic component $\{X_t\}$ can be estimated, or predicted, by the residual
$$\hat{X}= Y_t- \hat{\mu_t}$$

We call $\hat{X_t}$ the residual corresponding to the $t^{th}$ observation. If the trend model is reasonably correct, then the residuals should behave roughly like the true stochastic component, and various assumptions about the stochastic component can be assessed by looking at the residuals. If the stochastic component $\{Y_t\}$ is white noise, then the residuals should behave roughly like independent (normal) random variables with $zero^{(0)}$ mean and standard deviation $s$. Standardizing the residuals as $\hat{X}/s$ may be consider for plotting.
```{r, warning=FALSE, message=FALSE}
library(TSA)
data("tempdub")
month1<- season(tempdub)
model3<- lm(tempdub~month1)
summary(model3)

plot(y=rstudent(model3), x=as.vector(time(tempdub)),
xlab='Time', ylab='Standardized Residuals', type='o')
```

```{r}
plot(y=rstudent(model3), x=as.vector(time(tempdub)), xlab='Time', ylab='Standardized Residuals', type='l')

points(y=rstudent(model3), x=as.vector(time(tempdub)),  pch=as.vector(season(tempdub)))
```

```{r}
plot(y=rstudent(model3), x=as.vector(fitted(model3)), 
xlab='Fitted Trend Values', ylab='Standardized Residuals', type='n')

points(y=rstudent(model3), x=as.vector(fitted(model3)), pch=as.vector(season(tempdub)))
```

```{r}
hist(rstudent(model3), xlab='Standardized Residuals', main= "Histogram of Standardized Residuals from Seasonal Means Model")
```


```{r}
qqnorm(rstudent(model3), main= " Q-Q Plot: Standardized Residuals of Seasonal Means Model")
qqline(rstudent(model3))
```

### Sample Autocorrelation Function
Another very important diagnostic tool for examining dependence is the sample auto-correlation function. Consider any sequence of data $Y_1, Y_2,...,Y_n$ --- whether residuals, standardized residuals, original data, or some transformation of data. Tentatively assuming stationarity, we would like to estimate the autocorrelation function $\rho_k$ for a variety of lags $k=1,2,...$. The obvious way to do this is to compute the sample correlation between the pairs $k$ units apart in time. That is, among $(Y_1,Y_{1+k}), (Y_2,Y_{2+k}), (Y_3,Y_{3+k}),...,$ and $(Y_{n−k},Y_n)$. However, we modify this slightly, taking into account that we are assuming stationarity, which implies a common mean and variance for the series. With this in mind, we define the **sample autocorrelation function**, $r_k$, at $lag\,k$ as

$$r_k= \frac{\sum_{t=k+1}^n (Y_t-\bar{Y}) (Y_{t-k}-\bar{Y})}{\sum_{t=1}^n (Y_t-\bar{Y})^2}, \quad \text{for k = 1,2,...}$$
A plot of $r_k$ versus $lag\, k$ is often called a **correlogram**.

The hypotheses $H_0 : \rho_k=0$ can be rejected at the usual significance levels. The horizontal dashed lines, which are placed at $\pm 2/\sqrt{n}$ and none of these **auto-correlation** is cross thos line.
```{r}
data("tempdub")
month1<- season(tempdub)
model3<- lm(tempdub~ month1)

acf(rstudent(model3))
```

```{r}
data(rwalk)
model1<- lm(rwalk~ time(rwalk), data= rwalk)

plot(y=rstudent(model1), x=fitted(model1), ylab="Standardized Residuals", type="p")

acf(rstudent(model1))
```

we observed that the $lag\,1$ and $lag\,2$ autocorrelations exceed two standard errors above $zero^{(0)}$ and the $lag\,5$ and $lag\,6$ autocorrelations more than two standard errors below zero. This is not what we expect from a white noise process.

# MODELS FOR STATIONARY TIME SERIES

## Auto-Regressive Moving Average (ARMA) models
These models have assumed great importance in modeling real-world processes.

## General Linear Processes
A general linear process, $\{Y_t\}$, is one that can be represented as a weighted linear combination of present and past white noise terms as
$$Y_t= e_t+ \Psi_1e_{t-1}+ \Psi_2e_{t-2}+\cdots$$
with the assumption that $\sum_{i=1}^{\infty} \Psi_i^2< \infty$

An important nontrivial example to which we will return often is the case where the $\Psi’s$ form an exponentially decaying sequence 
$$\Psi_j= \phi^j$$
where $\phi$ is a number strictly between $−1$ and $+1$. Then
$$Y_t= e_t+ \phi e_{t-1}+ \phi^2e_{t-2}+\cdots$$
For this example.
$$E(Y_t)= E(e_t+ \phi e_{t-1}+ \phi^2e_{t-2}+\cdots)= 0$$
so that $\{Y_t\}$ has a constant mean of zero. Also
$$Var(Y_t)= Var(e_t+ \phi e_{t-1}+ \phi^2e_{t-2}+\cdots)$$
$$ = Var(e_t)+ \phi^2 Var(e_{t-1})+ \phi^4 Var(e_{t-2}) +\cdots$$
$$ = \sigma_e^2(1+ \phi^2+ \phi^4+\cdots)$$
$$Var(Y_t) = \frac{\sigma_e^2}{(1- \phi^2)} \quad\text{(by summing a geometric series))}$$
Furthermore,
$$Cov(Y_t, Y_{t-1})= Cov(e_t+ \phi e_{t-1}+ \phi^2e_{t-2}+\cdots, e_{t-1}+ \phi e_{t-2}+ \phi^2e_{t-3}+\cdots)$$
$$ = Cov(\phi e_{t-1},e_{t-1})+ Cov(\phi^2e_{t-2}, \phi e_{t-2}) +\cdots$$
$$ = \phi\sigma_e^2+ \phi^3\sigma_e^2+ \phi^5\sigma_e^2+ \cdots$$

$$ = \phi\sigma_e^2(1+ \phi^2+ \phi^4+\cdots)$$

$$Cov(Y_t, Y_{t-1}) = \frac{\phi\sigma_e^2}{(1- \phi^2)} \quad\text{(by summing a geometric series))}$$
Thus
$$cov(Y_t, Y_{t-1})= \frac{\Big[\frac{\phi\sigma_e^2}{1- \phi^2} \Big]}{\Big[\frac{\sigma_e^2}{1- \phi^2} \Big]}= \phi$$

In a similar manner, we can find $cov(Y_t, Y_{t-1})=\frac{\phi^k\sigma_e^2}{1- \phi^2}$ and thus $cov(Y_t, Y_{t-1})=\phi^k$

It is important to note that the process defined in this way is stationary---the auto-covariance structure depends only on time lag and not on absolute time.

## Moving Average Processes --- `MA(q)`
In this process, we change notation somewhat
$$Y_t= e_t- \theta_1e_{t-1}- \theta_2e_{t-2}- \cdots- - \theta_qe_{t-q}$$
We call such a series a **moving average of order q** and abbreviate the name to **MA(q)**.

### The First-Order Moving Average Process --- `MA(1)`
The model is 
$$Y_t= e_t- \theta e_{t-1}$$
It's **Mean**
$$E(Y_t)= E(e_t- \theta e_{t-1})$$
$$E(Y_t)= E(e_t)- \theta E(e_{t-1})$$
$$E(Y_t)= 0$$
and **Variance**
$$Var(Y_t)= Var(e_t- \theta e_{t-1})$$
$$Var(Y_t)= Var(e_t)- \theta^2 Var(e_{t-1})$$
$$= \sigma_e^2 + \theta^2 \sigma_e^2$$
$$Var(Y_t)= \sigma_e^2(1+ \theta^2)$$
and **Covariance**
$$Cov(Y_t, Y_{t-1})= Cov(e_t- \theta e_{t-1}, e_{t-1}- \theta e_{t-2})$$
$$= Cov(-\theta e_{t-1}, e_{t-1})$$
$$Cov(Y_t, Y_{t-1})= -\theta \sigma_e^2$$
and 
$$Cov(Y_t, Y_{t-2})= Cov(e_t- \theta e_{t-1}, e_{t-2}- \theta e_{t-3})$$
$$Cov(Y_t, Y_{t-2})= 0$$
Similarly,
$$Cov(Y_t, Y_{t-k})= 0 \quad for\,\, k \ge2$$
the process has no correlation beyond $lag\,1$.

In summary, for an **MA(1)** model $Y_t= e_t- \theta e_{t-1}$,
$$1.\quad E(Y_t)=0$$
$$2.\quad \gamma_0= Var(Y_t)= \sigma_e^2(1+ \theta^2)$$
$$3.\quad \gamma_1=  Cov(Y_t, Y_{t-1})= -\theta \sigma_e^2$$
$$4.\quad \rho_1=\frac{-\theta}{(1+ \theta^2)}$$
$$\gamma_k= \rho_k= 0 \quad for\,\, k \ge2$$
```{r, warning=FALSE, message=FALSE}
library(TSA)
data(ma1.2.s) 
plot(ma1.2.s, ylab=expression(Y[t]), type='o')
```

### The Second-Order Moving Average Process --- `MA(2)`
Consider the moving average process of order 2 :
$$Y_t= e_t- \theta_1e_{t-1}- \theta_2e_{t-2}$$
It's **Mean**
$$E(Y_t)= E(e_t- \theta_1e_{t-1}- \theta_2e_{t-2})$$
$$E(Y_t)= E(e_t)- \theta_1 E(e_{t-1})- \theta_2 E(e_{t-2})$$
$$E(Y_t)= 0$$
and **Variance**
$$Var(Y_t)= Var(e_t- \theta_1e_{t-1}- \theta_2e_{t-2})$$
$$Var(Y_t)= Var(e_t)- \theta_1^2 Var(e_{t-1})- \theta_2^2 Var(e_{t-2})$$
$$= \sigma_e^2 + \theta_1^2 \sigma_e^2+ \theta_2^2 \sigma_e^2$$
$$\gamma_0= Var(Y_t)= \sigma_e^2(1+ \theta_1^2+ \theta_2^2)$$
and **Covariance**
$$\gamma_1= Cov(Y_t, Y_{t-1})= Cov(e_t- \theta_1e_{t-1}- \theta_2e_{t-2}, e_{t-1}- \theta_1e_{t-2})- \theta_2e_{t-3})$$
$$= Cov(-\theta_1e_{t-1}, e_{t-1})+ Cov(-\theta_2e_{t-2}, -\theta_1e_{t-2})$$
$$= -\theta_1Cov(e_{t-1}, e_{t-1})+ \theta_1\theta_2Cov(e_{t-2}, e_{t-2})$$
$$Cov(Y_t, Y_{t-1})= [-\theta_1+ \theta_1\theta_2] \sigma_e^2$$
and 
$$\gamma_2= Cov(Y_t, Y_{t-2})= Cov(e_t- \theta_1e_{t-1}- \theta_2e_{t-2}, e_{t-2}- \theta_1e_{t-3})- \theta_2e_{t-4})$$
$$= Cov(-\theta_2e_{t-2}, e_{t-2})$$
$$\gamma_2= Cov(Y_t, Y_{t-2})= -\theta_2\sigma_e^2$$
for an **MA(2)** process
$$\rho_1= \frac{-\theta_1+ \theta_1\theta_2}{1+ \theta_1^2+ \theta_2^2}$$
$$\rho_2= \frac{-\theta_2}{1+ \theta_1^2+ \theta_2^2}$$
$$\rho_k=0\,\,; \quad \,\, k=3,4... 0R \,\,k \ge3$$
For specific case
$$Y_t= e_t- e_{t-1}+ 0.6Y_{t-2}$$
we have $\theta_1=1, \theta_2=0.6$
$$\rho_1= \frac{-1+(1)(-0.6)}{1+(1)^2+(0.6)^2}= \frac{-1.6}{3.6}= -0.678$$
$$\rho_2= \frac{0.6}{2.36}=0.254$$
```{r}
data(ma2.s)
plot(ma2.s, ylab=expression(Y[t]), type='o',
     main= "Time Plot of an MA(2) with 01=1 and 02=−0.6")
```

```{r}
plot(y=ma2.s, x= zlag(ma2.s), ylab= expression(Y[t]), xlab=expression(Y[t-1]), type="p") # rho1= -.678
```
```{r}
plot(y=ma2.s, x=zlag(ma2.s, 2), ylab = expression(Y[t]), xlab = expression(Y[t-2]), type="p") #rho2=0.254
```
```{r}
plot(y= ma2.s, x= zlag(ma2.s, 3), ylab= expression(Y[t]), xlab=expression(Y[t-3]), type="p") # rho3=0
```

### General Moving Average Process --- `MA(q)` 
For general **MA(q)** process
$$Y_t= e_t- \theta_1e_{t-1}- \theta_2e_{t-2}- \cdots- \theta_qe_{t-q}$$
**Mean :**
$$E(Y_t)= 0$$
**Variance :**
$$\gamma_0= Var(Y_t)= (1+ \theta_1^2+ \theta_2^2+ \cdots+ \theta_q^2)\sigma_e^2$$
and
$$
\rho_k= \frac{\gamma_k}{\gamma_0}= \begin{cases}
\frac{-\theta_k+ \theta_1\theta_{k+1}+ \theta_2\theta_{k+2}+ \cdots +\theta_q\theta_{k-q}}{1+ \theta_1^2+ \theta_2^2+ \cdots +\theta_q^2} \quad ; \text{for k=1,2,...,q} \\
0 \quad  ; \text{for k > q}
\end{cases}
$$

## Autoregressive Processes --- `AR(p)`
Autoregressive processes are as their name suggests—regressions on themselves. Specifically, a $p^{th}-order$ **autoregressive process** $\{Y_t\}$ satisfies the equation
$$Y_t= \phi_1Y_{t-1}+ \phi_2Y_{t-2}+ \cdots+ \phi_pY_{t-p}+ e_t$$
### The First-Order Autoregressive Process --- `AR(1)`
Assume the series is stationary and satisfies
$$Y_t= \phi Y_{t-1}++ e_t  --- (1)$$
$$E(Y_t)= 0$$
$$Var(Y_t)= \gamma_0= Var(\phi Y_{t-1}++ e_t)$$
$$Var(Y_t)= \gamma_0= \phi^2Var(Y_{t-1})+ Var(e_t)$$
$$Var(Y_t)= \gamma_0= \phi^2 \gamma_0+ \sigma_e^2$$
solving for $\gamma_0$, ,we get
$$\gamma_0= \frac{\sigma_e^2}{1-\phi^2}$$
Notice the immediate implication that $\phi^2<1$ or that $|\phi|<1$.

multiply both sides of equation (1) by $Y_{t−k}(k=1,2,...)$, we have
$$Y_tY_{t−k}= \phi Y_{t-1}Y_{t−k}+ e_tY_{t−k}$$
$$E(Y_tY_{t−k})= \phi E(Y_{t-1}Y_{t−k})+ E(e_tY_{t−k})$$
$$\gamma_k= \phi\gamma_{t-1}$$
Since the series is assumed to be stationary with zero mean, and since $e_t$ is independent of $Y_{t−k}$. 

Thus
$$E(e_tY_{t-k})= E(e_t)E(Y_{t-k})= 0$$
$$\gamma_k= \phi\gamma_{k-1} \quad; \text{for k=1,2,3,...}$$
Setting $k=1$, we get
$$\gamma_1= \phi\gamma_0= \phi\frac{\sigma_e^2}{1-\phi^2}$$
Setting $k=2$, we get
$$\gamma_2= \phi\gamma_1= \phi^2\frac{\sigma_e^2}{1-\phi^2}$$
In general
$$\gamma_k= \phi\gamma_{k-1}= \phi^k\frac{\sigma_e^2}{1-\phi^2}$$
and thus
$$\rho_k= \frac{\gamma_k}{\gamma_0}= \phi^k \quad; \text{for k=1,2,3,...}$$
for $k=0$
$$\rho_0= \frac{\gamma_0}{\gamma_0}= 1$$
for $k=1$
$$\rho_1= \frac{\gamma_1}{\gamma_0}= \phi$$
Since $|\phi|<1$, the magnitude of the autocorrelation function decreases exponentially as the number of $lags,\,k$, increases. 

* If $0<\phi<1$, all correlations are positive.
* If $-1<\phi<0$, the $lag\,1$ autocorrelation is negative $(\rho_1=\phi)$ and the signs of successive
autocorrelations alternate from positive to negative, with their magnitudes decreasing exponentially.
```{r}
data(ar1.s)
plot(ar1.s, ylab= expression(Y[t]), type="o")
```

```{r}
plot(ar1.s, x=zlag(ar1.s), ylab= expression(Y[t]), xlab = expression(Y[t-1]), type="p") # \rho1=0.9
```

```{r}
plot(ar1.s, x=zlag(ar1.s, 2), type="p", ylab=expression(Y[t]), xlab = expression(Y[t-2])) # rho2=0.9^2
```

```{r}
plot(ar1.s, x=zlag(ar1.s, 3), type="p", ylab=expression(Y[t]), xlab = expression(Y[t-3])) # rho2=0.9^3
```

#### The General Linear Process Version of the `AR(1)` Model
**AR(1)** Model is 
$$Y_t= \phi Y_{t-1}+ e_t \quad ---(1)$$
If we replacing the equation by $t-1$
$$Y_{t-1}= \phi Y_{t-2}+ e_{t-1} \quad ---(1)$$
Substituting $Y_{t-1}$ in equation 1
$$Y_t= \phi(\phi Y_{t-2}+ e_{t-1})+ e_t$$
$$Y_t= \phi^2Y_{t-2}+ \phi e_{t-1}+ e_t$$
$$Y_t= e_t+ \phi e_{t-1}+ \phi^2Y_{t-2}$$
If we repeat this substitution into the past, say $k−1$ times, we get
$$Y_t= e_t+ \phi e_{t-1}+ \phi^2 e_{t-2}+ \cdots+ \phi^{k-1} e_{t-k+1}+ + \phi^k Y_{t-k}$$
Assuming $|\phi|<1$ and letting $k$ increase without bound, we should obtain the infinite series representation
$$Y_t= e_t+ \phi e_{t-1}+ \phi^2 e_{t-2}+$$
This is the general form form of the equation
$$Y_t= e_t+ \Psi_1 e_{t-1}+ \Psi_2 e_{t-2}+ \cdots$$
with 
$$\Psi_j= \phi^j$$

**Stationarity of an AR(1) Process is that** $|\phi|<1$.

### The Second-Order Autoregressive Process --- `AR(2)`
Now consider the series satisfying
$$Y_t= \phi_1Y_{t-1}+ \phi_2Y_{t-2}+ e_t$$
where, as usual, we assume that $e_t$ is independent of $Y_{t−1}, Y_{t−2}, Y_{t−3},...$. To discuss stationarity, we introduce the **AR characteristic polynomial**
$$\phi(x)= 1- \phi_1x- \phi_2x^2$$
and the corresponding **AR characteristic equation**
$$1- \phi_1x- \phi_2x^2= 0$$
We recall that a quadratic equation always has two roots (possibly complex).

#### Stationarity of the `AR(2)` Process
In the second-order case, the roots of the quadratic characteristic equation are easily found to be
$$\frac{\phi_1 \pm \sqrt{\phi_1^2+ 4\phi_2}}{-2\phi_2}$$
For stationarity, we require that these roots exceed 1 in absolute value, this will be true if and only if three conditions are satisfied :

i. $\phi_1+ \phi_2<1$

ii. $\phi_2- \phi_1<1$

iii. $|\phi_1|<1$

For **AR(1)** process, the AR Characteristic Equation is just
$$1- \phi x= 0$$
with root $1/\phi$ which exceed $1$ in absolute value iff $|\phi|<1$.

#### The Autocorrelation Function for the `AR(2)` Process
To derive the autocorrelation function for the AR(2) case, we take the defining recursive relationship of equation
$$Y_t= \phi_1Y_{t-1}+ \phi_2Y_{t-2}+ e_t$$
multiplying  both the sides by $Y_{t-k}$ and get expectations, we have
$$Y_tY_{t-k}= \phi_1Y_{t-1}Y_{t-k}+ \phi_2Y_{t-2}Y_{t-k}+ e_tY_{t-k}$$
$$E(Y_tY_{t-k})= \phi_1E(Y_{t-1}Y_{t-k})+ \phi_2E(Y_{t-2}Y_{t-k})+ E(e_tY_{t-k})$$
$$\gamma_k= \phi_1\gamma_{k-1}+ \phi_2\gamma_{k-2} \quad --- (A)$$
Dividing equation (A) by $\gamma_0$
$$\frac{\gamma_k}{\gamma_0}= \phi_1 \frac{\gamma_{k-1}}{\gamma_0}+ \phi_2 \frac{\gamma_{k-2}}{\gamma_0}$$
$$\rho_k= \phi_1\rho_{k-1}+ \phi_2\rho_{k-2} \quad ---(B) \quad \text{for k=1,2,...}$$
Equations (A) and/or (B) are usually called the **Yule-Walker equations**

Setting $k=1$, we get
$$\rho_1= \phi_1\rho_0+ \phi_2\rho_{-1}$$
$$\rho_1= \phi_1+ \phi_2\rho_1$$
and so
$$\rho_1= \frac{\phi_1}{1-\phi_2}$$
Using the now known values for $\rho_1(and\,\, \rho_0)$, equation(B) can be used with $k=2$ to obtain
$$\rho_2= \phi_1\rho_1+ \phi_2\rho_0$$
$$\rho_2= \frac{\phi_1^2+ \phi_2(1-\phi_2)}{(1-\phi_2)}$$
Successive values of ρk may be easily calculated numerically from the recursive relationship of Equation (B).
```{r}
data(ar2.s)
plot(ar2.s,ylab=expression(Y[t]),type='o')
```

### Varicance for the `AR(2)` Model 
Consider the auto-regression 
$$Y_t= \phi_1Y_{t-1}+ \phi_2Y_{t-2}+ e_t$$
$$Var(Y_t)= \phi_1^2Var(Y_{t-1})+ \phi_2^2Var(Y_{t-2})+ 2\phi_1\phi_2Cov(Y_{t-1}Y_{t-2})+ e_t$$
$$\gamma_0= (\phi_1^2+ \phi_2^2)\gamma_0+ 2\phi_1\phi_2\gamma_1+ \sigma_e^2 \quad --- (i)$$
Setting $k=1$ in equation(A) gives a second linear equation for $\gamma_0$ and $\gamma_1$, $\gamma_1= \phi_1\gamma_0+ \phi_2\gamma_1$, which can be solved simultaneously with euation $i$ to obtain
$$\gamma_0= \frac{(1-\phi_2)\sigma_e^2}{(1-\phi_2)(1-\phi_1^2-\phi_2^2)-2\phi_2\phi_1^2}$$
$$\gamma_0= \Big(\frac{1-\phi_2}{1+\phi_2}\Big)\frac{\sigma_e^2}{(1-\phi_2)^2-\phi_1^2}$$

## General Auto-Regressive Process
Consider the $p^{th}$-order autoregressive model
$$Y_t= \phi_1Y_{t-1}+ \phi_2Y_{t-2}+ \cdots+ \phi_pY_{t-p}+ e_t$$
with AR characteristic polynomial
$$\phi(x)= 1-\phi_1x- \phi_2x^2- \cdots- \phi_px^p$$
and corresponding AR characteristic equation
$$1-\phi_1x- \phi_2x^2- \cdots- \phi_px^p= 0$$
The necessary condition for a stationary is
$$\phi_1+ \phi_2+ \cdots+ \phi_p<1$$
and
$$|\phi_p|<1$$

## Mixed Autoregressive Moving Average Model --- `ARMA(p,q)`
If we assume that the series is partly autoregressive and partly moving average, we obtain a quite general time series model. In general, if
$$Y_t= \phi_1Y_{t-1}+ \phi_2Y_{t-2}+ \cdots+ \phi_pY_{t-p}+ e_t- \theta_1e_{t-1}- \theta_2e_{t-2}- \cdots- \theta_qe_{t-q}$$
we say that $\{Y_t\}$ is a mixed **autoregressive moving average** process of orders $p$ and $q$, respectively. We abbreviate the name to $ARMA(p,q)$.

### ARMA(1, 1) Model 
The defining equation can be written 
$$Y_t= \phi Y_{t-1}+ e_t- \theta e_{t-1} \quad --- (ii)$$
To derive Yule-Walker type equations, we first note that
$$E(e_tY_t)= E[e_t(\phi Y_{t-1}+ e_t- \theta e_{t-1})]$$
$$E(e_tY_t)= \sigma_e^2$$
and
$$E(e_{t-1}Y_t)= E[e_{t-1}(\phi Y_{t-1}+ e_t- \theta e_{t-1})]$$
$$E(e_{t-1}Y_t)= \phi\sigma_e^2- \theta\sigma_e^2$$
$$E(e_{t-1}Y_t)= (\phi- \theta)\sigma_e^2$$
If we multiply Equation $ii$ by $Y_{t−k}$ and take expectations, we have
$$Y_tY_{t−k}= \phi Y_{t-1}Y_{t−k}+ e_tY_{t−k}- \theta e_{t-1}Y_{t−k}$$
$$E(Y_tY_{t−k})= \phi E(Y_{t-1}Y_{t−k})+ E(e_tY_{t−k})- \theta E(e_{t-1}Y_{t−k})$$
For $k=0$
$$\gamma_0= \phi\gamma_1+ \sigma_e^2- \theta(\phi-\theta)\sigma_e^2$$
$$\gamma_0= \phi\gamma_1+ [1-\theta(\phi-\theta)]\sigma_e^2$$
$$\gamma_1= \phi\gamma_1- \theta\sigma_e^2$$
$$\gamma_k= \phi_{k-1} \quad for\,\, k \ge 2$$
Solving the first two equations yields
$$\gamma_0= \frac{1-2\theta\phi+ \theta^2}{1-\phi^2}\sigma_e^2$$
and solving the simple recursion gives
$$\frac{(1-\theta\phi)(\phi-\theta)}{1-2\theta\phi+ \theta^2}\phi^{k-1} \quad for\,\, k \ge 1$$
**Remark :** Note that this autocorrelation function decays exponentially as the lag k increases. The damping factor is $\phi$, but the decay starts from initial value $\rho_1$, which also depends on $\theta$. This is in contrast to the AR(1) autocorrelation, which also decays with damping factor $\phi$ but always from initial value $\rho_0=1$. **For example**, if $\phi=0.8$ and $\theta=0.4$, then

* $\rho_1=0.523$, 
* $\rho_2=0.418$, 
* $\rho_3=0.335$, 
* and so on. 

# Models for Non-Stationary Time Series --- `ARIMA`
Any time series without a constant mean over time is nonstationary. Models of the form
$$Y_t= \mu_t+ X_t$$
where $\mu_t$ is a non-constant mean function and $X_t$ is a zero-mean.

As an example consider the monthly price of a barrel of crude oil from January $1986$ through January $2006$.
```{r, message=FALSE, warning=FALSE}
library(TSA)
data("oil.price")
plot(oil.price, ylab= "Price Per Barrel", type= "l", main= "Non-Stationary Model")
```

The series displayed considerable variation specially since $2001$ and a stationary model doesn't seen to be reasonable.

## Stationarity Through Differencing --- `ARIMA Models`
A time series $\{Y_t\}$ is said to follow an **integrated autoregressive moving average** model if the $d^{th}$ difference $W_t= \nabla^dY_t$ is a stationary ARMA process. If $\{W_t\}$ follows an **ARMA(p,q) model**, we say that $\{Y_t\}$ is an **ARIMA(p,d,q) process**. Fortunately, for practical purposes, we can usually take $d=1$ or at most $2$.

Consider then an **ARIMA(p,1,q) process**. With $W_t=Y_t− Y_{t−1}$, we have
$$W_t= \phi_1W_{t-1}+ \phi_2W_{t-2}+ \cdots+ \phi_pW_{t-p}+ e_t- \theta_1e_{t-1}- \theta_2e_{t-2}- \cdots- - \theta_qe_{t-q}$$
or, in terms of the observed series,
$$Y_t-Y_{t-1}= \phi_1(Y_{t-1}-Y_{t-2})+ \phi_2(Y_{t-2}-Y_{t-3})+ \cdots+ \phi_p(Y_{t-p}-Y_{t-p-1})+ e_t- \theta_1e_{t-1}- \theta_2e_{t-2}- \cdots- \theta_qe_{t-q}$$
which we may rewrite as
$$Y_t= (1+\phi_1)Y_{t-1}+ (\phi_2-\phi_1)Y_{t-2}+ (\phi_3-\phi_2)Y_{t-3}+ \cdots+ (\phi_p-\phi_{p-1})Y_{t-p}- \phi_pY_{t-p-1}+ e_t- \theta_1e_{t-1}- \theta_2e_{t-2}- \cdots- \theta_qe_{t-q}$$
We call this the **difference equation form** of the model. 

```{r}
data("oil.price")
plot(oil.price, ylab= "Price Per Barrel", type= "l", main= "Non-Stationary Model")

plot(diff(log(oil.price)), ylab= "Change in log(Price)", type= "l", main= "Stationary Model")
```

It is seen that difference are stationary.

### Constant Terms in ARIMA Models
For an $ARIMA(p,d,q)$ model, $\nabla^dY_t= W_t$ is a stationary $ARMA(p,q)$ process. Our standard assumption is that stationary models have a zero mean; that is, we are actually working with deviations from the constant mean. A nonzero constant mean, $\mu$, in a stationary ARMA model $\{W_t}$ can be accommodated in either of two ways. We can assume that
$$W_t-\mu= \phi_1(W_{t-1}-\mu)+ \phi_2(W_{t-2}-\mu)+ \cdots+ \phi_p(W_{t-p}-\mu)+ e_t- \theta_1e_{t-1}- \theta_2e_{t-2}- \cdots- \theta_qe_{t-q}$$
Alternatively, we can introduce a constant term $\theta_0$ into the model as follows :
$$W_t= \theta_0+ \phi_1W_{t-1}+ \phi_2W_{t-2}+ \cdots+ \phi_pW_{t-q}+ e_t- \theta_1e_{t-1}- \theta_2e_{t-2}- \cdots- \theta_qe_{t-q}$$
Taking expected values on both sides of the latter expression, we find that
$$E(W_t)= \theta_0+ \phi_1E(W_{t-1})+ \phi_2E(W_{t-2})+ \cdots+ \phi_pE(W_{t-q})+ E(e_t)- \theta_1E(e_{t-1})- \theta_2E(e_{t-2})- \cdots- \theta_qE(e_{t-q})$$
$$\mu= \theta_0+ \phi_1\mu+ \phi_2\mu+ \cdots+ \phi_p\mu+ 0+0 \cdots+ 0$$

$$\mu= \theta_0+ (\phi_1+ \phi_2+ \cdots+ \phi_p)\mu$$

$$\mu= \frac{\theta_0}{1- (\phi_1+ \phi_2+ \cdots+ \phi_p)}$$

or, conversely, that
$$\theta_0= \mu[1- (\phi_1+ \phi_2+ \cdots+ \phi_p)]$$

## Other Transformation

### Log Transformation --- `log()`
We have seen how differencing can be a useful transformation for achieving stationarity. However, the logarithm transformation is also a useful method in certain circumstances.

We frequently encounter series where increased dispersion seems to be associated with higher levels of the series—the higher the level of the series, the more variation there is around that level and conversely.

Specifically, suppose that $Y_t>0$ for all $t$ and that
$$E(Y_t)= \mu_t \quad and \quad \sqrt{Var(Y_t)}= \mu_t\sigma$$
Then
$$E[log(Y_t)]\approx log(\mu_t) \quad and \quad Var(log(Y_t)) \approx \sigma^2$$
These results follow from taking expected values and variances of both sides of the (Taylor) expansion
$$log(Y_t)\approx log(\mu_t)+ \frac{Y_t- \mu_t}{\mu_t}$$
$$E(log(Y_t))\approx E(log(\mu_t))+ \frac{E(Y_t- \mu_t)}{\mu_t}$$
$$E(log(Y_t))\approx log(\mu_t)$$
$$Var(log(Y_t))\approx Var(log(\mu_t))+ \frac{1}{\mu_t^2}Var(Y_t- \mu_t)$$
$$Var(log(Y_t))\approx 0+ \frac{1}{\mu_t^2}\mu_t^2 \sigma^2$$
$$Var(log(Y_t))\approx \sigma^2$$
In other words, if standard deviation of the series so proportional to the level of the series, then transforming to logarithms will produce a series with approximately constant variance over time. 

### Percentage Changes and Logarithms --- `returns`
Suppose $Y_t$ tends to have relatively stable percentage changes from one time period to the next. Specifically, assume that
$$Y_t= (1+ X_t)Y_{t-1}$$
where $100X_t$ is the percentage change (possibly negative) from $Y_{t−1}$ to $Y_t$. Then
$$log(Y_t)- log(Y_{t-1})= log\Big(\frac{Y_t}{Y_{t-1}}\Big)= log(1+ X_t)$$
$$\Delta[log(Y_t)] \approx X_t \quad if \,\, |X_t|<0.20$$

will be relatively stable and perhaps well-modeled by a stationary process. Notice that we take logs first and then compute first differences —-- the order does matter. In financial literature, the differences of the (natural) logarithms are usually called **returns**.


```{r}
data(electricity)
plot(electricity)

plot(log(electricity), ylab= "Log(electricity)", main= "Logarithms of Electricity Values")

plot(diff(log(electricity)), 
ylab= "Difference of Log(electricity)", main= " Difference of Logarithms for Electricity Values")
```

### Power Transformations
A flexible family of transformations, the **power transformations**, was introduced by Box and Cox (1964). For a given value of the parameter $\lambda$, the transformation is defined by
$$g(x)= \begin{cases}
\frac{x^{\lambda}-1}{\lambda} \quad for \,\, \lambda \ne 0 \\
log(x) \quad for \,\, \lambda = 0
\end{cases}
$$
$$\therefore \lim_{\lambda \to 0}\frac{x^{\lambda}-1}{\lambda}= log(x) $$

Note that $\lambda= \frac{1}{2}$ produce square root transformation useful for Poisson data and $\lambda= -1$ corresponds to reciprocal transformation.

Now the question arises how to get the value of $\lambda$ which decides what transformation is to be used.
```{r, warning=FALSE, message=FALSE}
BoxCox.ar(electricity)
```

when $\lambda=0 \Rightarrow$ logarithmic transformation is required. This suggests logarithmic transformation for the data.

when $\lambda=1 \Rightarrow$ No transformation.

## The Backshift Operator
The Backshift Operator is linear since
$$B(aY_t+ bX_t+ c)= aBY_t+ bBX_t+ c$$

### Moving Average Model --- `MA()` Model
Consider the MA(1) Model. In terms of $B$, we can write as
$$Y_t= e_t- \theta e_{t-1}$$
$$= (1-\theta B)e_t$$
$$Y_t= \theta(B)e_t$$
where $\theta(B)$ is the MA characteristic polynomial estimated at $B$.

Since $BY_t$ is itself a time series, it is meaningful to consider $BBY_t$. But clearly $BBY_t= BY_{t−1}= Y_{t−2}$, and we can write
$$B^2Y_t= Y_{t-2}$$
More generally, we have
$$B^mY_t= Y_{t-m}$$
for any positive integer $m$. For a general $MA(q)$ model, we can then write
$$Y_t= e_t- \theta_1e_{t-1}- \theta_2e_{t-2}- \cdots- \theta_qe_{t-q}$$
$$= e_t- \theta Be_t- \theta B^2e_t- \cdots- \theta_q B^qe_q $$
$$Y_t= (1- \theta_1B- \theta_2B^2- \cdots- \theta_qB^q)e_t$$
OR
$$Y_t= \theta(B)e_t$$
where, again, $\theta(B)$ is the MA characteristic polynomial evaluated at $B$.

### Auto-Regressive Models --- AR(p) Models
For auto-regressive models $AR(p)$, we first move all of the terms involving $Y$ to the left-hand side
$$Y_t- \phi_1Y_{t-1}- \phi_2Y_{t-2}- \cdots- \phi_pY_{t-p}= e_t$$
and then write
$$Y_t- \phi_1BY_t- \phi_2B^2Y_t- \cdots- \phi_pB^pY_t= e_t$$
or
$$(1- \phi_1B- \phi_2B^2- \cdots- \phi_pB^p)Y_t= e_t$$
which can be expressed as
$$\phi(B)Y_t= e_t$$
where $\phi(B)$ is the AR characteristic polynomial evaluated at $B$.

### ARMA(p, q) Models
Combining the two, the general ARMA(p,q) model may be written compactly as
$$\phi(B)Y_t= \theta(B)e_t$$
Differencing can also be conveniently expressed in terms of $B$. We have
$$\nabla Y_t= Y_t- Y_{t-1}= Y_t-BY_t$$
$$\nabla Y_t= (1-B)Y_t$$
with second differences given by
$$\nabla^2 Y_t= (1-B)^2Y_t$$
Effectively, $\nabla= (1-B)$ and $\nabla^2= (1-B)^2$.

The general $ARIMA(p,d,q)$ model is expressed concisely as
$$\phi(B)(1-B)^dY_t= \theta(B)e_t$$

# Parameter Estimation 
This chapter deals with the problem of estimating the parameters of an $ARIMA$ model based on the observed time series $Y_1, Y_2,..., Y_n$. With regard to non-stationarity, since the $d^{th}$ difference of the observed series is assumed to be a stationary $ARMA(p,q)$ process, we need only concern ourselves with the problem of estimating the parameters in such stationary models.

## The Method of Moments
1. Methods of Moments
2. Methods of Least Square Estimation
3. Methods of Maximum Likelihood Estimation
4. Bayesian Methods.

### 1. Methods of Moments
The method consists of equating sample moments to corresponding theoretical moments and solving the resulting equations to obtain estimates of any unknown parameters.

#### Autoregressive Models --- `AR(p)` Model
Consider first the $AR(1)$ case. For this process, we have the simple relationship $\rho_1= \phi$.

In the method of moments, $\rho_1$ is equated to $r_1$, the $lag\,\,1$ sample autocorrelation. Thus we can estimate $\phi$ by
$$\hat{\phi}= r_1$$
Now consider the $AR(2)$ case. The relationships between the parameters $\phi_1$ and $\phi_2$ and various moments are given by the *Yule-Walker* equations
$$\rho_1= \phi_1+ \rho_1\phi_2$$
$$\rho_2= \rho_1\phi_1+ \phi_2$$
Thus method of moments gives
$$r_1= \phi_1+ r_1\phi_2$$
$$r_2= r_1\phi_1+ \phi_2$$
which are then solved to obtain
$$\hat{\phi_1}= \frac{r_1(1-r_1)}{1-r_1^2}$$
and 
$$\hat{\phi_2}= \frac{r_2- r_1^2}{1-r_1^2} $$
The general $AR(p)$ Model
$$\phi_1+ r_1\phi_2+ r_2\phi_3+ \cdots+ r_{p-1}\phi_p= r_1$$
$$r_1\phi_1+ \phi_2+ r_1\phi_3+ \cdots+ r_{p-2}\phi_p= r_2$$
$$r_{p-1}\phi_1+ r_{p-2}\phi_2+ r_{p-3}\phi_3+ \cdots+ \phi_p= r_p$$
The estimates obtained in this way are also called **Yule-Walker estimates**.

#### Moving Average Models --- `MA(q)` Model
$MA(q)$ are quite complicated using method of moments.

#### Mixed Model --- `ARMA(1,1)` Model
We consider only the $ARMA(1,1)$ case. Recall Equation $$\rho_k= \frac{(1-\theta\phi)(\phi-\theta)}{1-2\theta\phi+\theta^2} \phi^{k-1} \quad for\,\, k\ge1$$
Noting that $\rho_2/\rho_1= \phi$, we can first estimate $\phi$ as
$$\hat{\phi}= \frac{r_2}{r_1}$$
Having done so, we can then use
$$r_1= \frac{(1-\theta\hat{\phi})(\hat{\phi}-\theta)}{1-2\theta\hat{\phi}+\theta^2}$$
then we shall solve it for $\theta$ to get $\hat{\theta}$.

#### Estimates of the Noise Variance
In all cases, we can first estimate the process variance, $\gamma_0= Var(Y_t)$, by the sample variance
$$s^2= \frac{1}{n-1} \sum_{t=1}^n(Y_t- \bar{Y})^2$$
and use the relationship fro $AR(p)$
$$\hat{\sigma_e^2}= (1- \hat{\phi_1}r_1- \hat{\phi_2}r_2- \cdots- \hat{\phi_p}r_p)s^2$$
In particular, for an $AR(1)$ process,
$$\hat{\sigma_e^2}= (1-r_1^2)s^2 \quad \{\because \hat{\phi_1}= r_1$$
For the $MA(q)$ case, we have, using Equation
$$\hat{\sigma_e^2}= \frac{s^2}{1+ \hat{\theta_1^2}+ \hat{\theta_2^2}+ \cdots+ \hat{\theta_q^2}}$$
For the $ARMA(1,1)$ process
$$\hat{\sigma_e^2}= \frac{1- \hat{\phi^2}}{1- 2\hat{\phi}\hat{\theta}+ \hat{\theta^2}}s^2$$

### 2. Methods of Least Square Estimation
Consider auto-regressive model
$$Y_t- \mu= \phi(Y_{t-1}- \mu)+ e_t$$
$$\Rightarrow e_t= [Y_t- \mu- \phi(Y_{t-1}- \mu)]$$
$$\Rightarrow e_t^2= [Y_t- \mu- \phi(Y_{t-1}- \mu)]^2$$
$$\Rightarrow \sum_{t=2}^n e_t^2= \sum_{t=2}^n[Y_t- \mu- \phi(Y_{t-1}- \mu)]^2$$
$$\Rightarrow \sum_{t=2}^n e_t^2= S_c(\phi, \mu)$$
We have to minimize this $S_c(\mu, \phi)$ with respect to $\mu$ and $\phi$
$$\frac{\partial S_c(\mu, \phi)}{\partial\mu}= 0 \quad we \,\, have$$
$$\frac{\partial S_c(\mu, \phi)}{\partial\mu}=  \sum_{t=2}^n 2[Y_t- \mu- \phi(Y_{t-1}- \mu)](-1+ \phi)= 0$$
$$\Rightarrow \hat{\mu}= \frac{1}{(n-1)(1- \phi)} \Bigg[\sum_{t=2}^n Y_t- \phi\sum_{t=2}^n Y_{t-1} \Bigg]$$
Now for large $n$
$$\frac{1}{(n-1)}\sum_{t=2}^n Y_t \approx \frac{1}{(n-1)}\sum_{t=2}^n Y_{t-1} \approx \bar{Y}$$
Thus, regardless of the value of $\phi$
$$\hat{\mu} \approx \frac{1}{1-\phi}(\bar{Y}- \phi\bar{Y})= \bar{Y}$$
Consider now the minimization
$$S_c(\mu, \phi)= \sum_{t=2}^n[(Y_t- \bar{Y})- \phi(Y_{t-1}- \bar{Y})]^2$$
differentiate with respect to $\phi$, we have
$$\frac{\partial S_c(\phi, \mu)}{\partial\phi}= \sum_{t=2}^n -2[(Y_t- \bar{Y})- \phi(Y_{t-1}- \bar{Y})](Y_{t-1}- \bar{Y})= 0$$
$$\hat{\phi}= \frac{\sum_{t=2}^n(Y_t- \bar{Y})(Y_{t-1}- \bar{Y})}{\sum_{t=2}^n(Y_{t-1}- \bar{Y})^2}$$
Except for one term missing in the denominator, namely $(Y_n- \bar{Y})^2$, this is the same as $r_1$. The lone missing term is negligible for stationary processes, and thus the least squares and method-of-moments estimators are nearly identical, especially for large samples.

### Maximum Likelihood Estimation --- MLE
* **Likelihood Function :** When joint density of $Y_1, Y_2,..., Y_n$ is treated as a function of parameters, it is termed as likelihood function.

* **Maximum Likelihood Estimator :** The estimate of the parameters which maximizes likelihood function is termed as maximum likelihood estimator.

#### Estimation of AR(1) Model
We know that $e_t \sim N(0, \sigma_e^2)$. Thus, the probability density function for each $e_t$ is then 
$$(2\pi \sigma_e^2)^{-1/2} exp \Big(\frac{-e_t^2}{2\sigma^2_e}\Big) \quad for -\infty < e_t < \infty$$
by independence, the joint pdf for $e_2, e_3,...,e_n$ is
$$(2\pi \sigma_e^2)^{-(n-1)/2} exp \Big(-\frac{1}{2\sigma^2_e}\sum_{t=2}^n e_t^2 \Big)$$
Now consider
$$Y_2- \mu= \phi(Y_1- \mu)+ e_2$$
$$Y_3- \mu= \phi(Y_2- \mu)+ e_3$$
$$\vdots$$
$$Y_n- \mu= \phi(Y_{n-1}- \mu)+ e_n$$
Therefore
$$f(Y_2,Y_3,...,Y_n|Y_1)= (2\pi \sigma_e^2)^{-(n-1)/2} \times exp \Bigg\{-\frac{1}{2\sigma^2_e}\sum_{t=2}^n \Big[(Y_t- \mu)- \phi(Y_{t-1}- \mu) \Big]^2 \Bigg\} $$
We know that 
$$Y_1 \sim N\Big(\mu, \frac{\sigma_e^2}{1-\phi^2} \Big)$$
Therefore
$$L(\phi, \mu, \sigma_e^2)= (2\pi\sigma_e^2)^{-n/2} (1-\phi^2)^{1/2} exp\Big[-\frac{1}{2\sigma_e^2}S(\phi, \mu) \Big]$$
where,
$$S(\phi, \mu)= \sum_{t=2}^n \big[(Y_t- \mu)- \phi(Y_{t-1}- \mu)\big]^2+ (1- \phi^2)(Y_1- \mu)$$
The function $S(\phi, \mu)$ is called the **unconditional sum-of-squares function**.

The Likelihood function 
$$l(\phi, \mu, \sigma_e^2)= -\frac{n}{2}log(2\pi)- \frac{n}{2}log(\sigma_e^2)+ \frac{1}{2}log(1- \phi^2)- \frac{1}{2\sigma_e^2} S(\phi, \mu)$$

* This function could be maximize using `optim()` function of $R$.
* Note also that $S(\phi, \mu)= S_c(\phi, \mu)+ (1- \phi^2)(Y_t-\mu)^2$

## Properties of the Estimates
For large $n$, the estimators are approximately unbiased and normally distributed.

The variances and correlations are as follows :
$$AR(1): Var(\hat{\phi}) \approx \frac{1-\phi^2}{n}$$

$$
AR(2): 
\begin{cases}
Var(\hat{\phi_1})\approx Var(\hat{\phi_2})\approx \frac{1-\phi_2^2}{n}\\
Corr(\hat{\phi_1}, \hat{\phi_2})\approx -\frac{\phi_1}{1-\phi_2}= -\rho_1
\end{cases}
$$

$$MA(1): Var(\hat{\theta}) \approx \frac{1-\theta^2}{n}$$

$$
MA(2): 
\begin{cases}
Var(\hat{\theta_1})\approx Var(\hat{\theta_2})\approx \frac{1-\theta_2^2}{n}\\
Corr(\hat{\theta_1}, \hat{\theta_2})\approx -\frac{\theta_1}{1-\theta_2}
\end{cases}
$$

$$
ARMA(1, 1): 
\begin{cases}
Var(\hat{\phi})\approx \Big[\frac{1-\phi^2}{n}\Big]\Big[\frac{1-\phi\theta}{\phi-\theta}\Big]^2 \\

Var(\hat{\theta})\approx \Big[\frac{1-\theta^2}{n}\Big]\Big[\frac{1-\phi\theta}{\phi-\theta}\Big]^2 \\

Corr(\hat{\phi}, \hat{\theta})\approx \frac{\sqrt{(1-\phi^2)(1-\theta^2)}}{1-\phi\theta}
\end{cases}
$$

### Parameter Estimates For Simulated Data Using AR(1) Model
```{r}
data(ar1.s)
ar(ar1.s, order.max=1, AIC=F, method='yw')
ar(ar1.s, order.max=1, AIC=F, method='ols')
ar(ar1.s, order.max=1, AIC=F, method='mle')
```

### Parameter Estimates For Simulated Data Using AR(2) Model
```{r}
data(ar1.2.s)
ar(ar1.2.s, order.max=1, AIC=F, method='yw')
ar(ar1.2.s, order.max=1, AIC=F, method='ols')
ar(ar1.2.s, order.max=1, AIC=F, method='mle')
```

### Canadian Hare Abendance Series --- Hare
Consider the canadian hare abendance series as before we base our modelling on the square root og original abendance number based on partial auto-correlation function we see $AR(3)$ Model was suitable.
```{r}
data("hare")
BoxCox.ar(hare)
```
This output suggest should be square root transformation and using `BoxCox.ar()`. Suggest that it should be square root transformation.
```{r}
pacf(hare^0.5)
```
* $\Rightarrow p=3$
* $\Rightarrow AR(3)$ Model should be fitted for `hare` data.

### Fitting of ARIMA Model --- `AR(3)`
```{r}
data("hare")
arima(sqrt(hare), order= c(3, 0, 0))
```

Here we see that $\hat{\phi_1}=1.0519$, $\hat{\phi_2}=−0.2292$, and $\hat{\phi_3}=−0.3930$. We also see that the estimated noise variance is $\hat{\sigma_e^2}=1.066$ and ratio(z-ratio) $\frac{\hat{\phi_1}}{se(\hat{\phi_1})}= \frac{1.0519}{0.1877}= 3.6041 \Rightarrow significant$, $\frac{\hat{\phi_2}}{se(\hat{\phi_2})}= \frac{-0.2292}{0.2942}= -0.779 \Rightarrow not-significant$ and $\frac{\hat{\phi_3}}{se(\hat{\phi_3})}= \frac{-0.3931}{0.1915}= -2.0527 \Rightarrow significant$

The estimated model would be written
$$\sqrt{Y_t}-5.6923= 1.0519(\sqrt{Y_{t-1}}- 5.6923) –0.2292(\sqrt{Y_{t-2}} –5.6923)-
0.3930(\sqrt{Y_{t-3}}- 5.6923)+ e_t$$
or
$$\sqrt{Y_t}= 3.25+ 1.0519\sqrt{Y_{t-1}}– 0.2292\sqrt{Y_{t-2}}– 0.3930\sqrt{Y_{t-3}}+ e_t$$
### Example of `MA(1)` Model
As a last example, we return to the oil price series. The sample ACF shown in the following which suggested an MA(1) model on the differences of the logs of the prices.
```{r}
data(oil.price)

arima(log(oil.price),order=c(0,1,1),method='CSS') # Conditional Sum of Square

arima(log(oil.price),order=c(0,1,1),method='ML') # Maximum Likelihood
```

## Bootstrapping ARIMA Models
```{r}
data("hare")
res<- arima(sqrt(hare), order = c(3,0,0))

coef.cond.norm<- arima.boot(res, cond.boot=T, is.normal=T, B=1000, init= sqrt(hare))
# coef.cond.norm
dim(coef.cond.norm)

head(coef.cond.norm)

temp<- apply(coef.cond.norm, 2, function(x) {quantile(x, c(0.025, 0.975))})
temp

temp_mean<- apply(coef.cond.norm, 2, mean)  
temp_mean

temp_median<- apply(coef.cond.norm, 2, median)
temp_median

signif(temp, 3)

round(temp, 3)

round(rbind(temp_median, temp), 3)
```

## MODEL DIAGNOSTICS
```{r}
data(color)

m1.color<- arima(color, order=c(1,0,0)); m1.color

plot(rstandard(m1.color), ylab= 'Standardized Residuals',type='o')
abline(h=0)
```

###  Standardized Residuals from Log Oil Price IMA(1,1) Mode
**IMA : Integrated Moving Average**
```{r}
data(oil.price)

m1.oil<-  arima(log(oil.price), order= c(0,1,1))
m1.oil

plot(rstandard(m1.oil), ylab= 'Standardized residuals', type='l')
abline(h=0)
```

### Normality of the Residuals
```{r}
data("color")

m1.color<- arima(color, order= c(1,0,0))

qqnorm(residuals(m1.color), main= "QQ Plot of Residuals from AR(1) Model")
qqline(residuals(m1.color))
```

```{r}
data(hare)
m1.hare<- arima(sqrt(hare), order= c(3,0,0))

qqnorm(residuals(m1.hare), main= "QQ Plot of Residuals from AR(3) Model")
qqline(residuals(m1.hare))
```

```{r}
hist(window(rstandard(m1.co2), start=c(1995,2)), xlab= 'Standardized Residuals')

# QQnormal
qqnorm(window(rstandard(m1.co2), start=c(1995,2)))
qqline(window(rstandard(m1.co2), start=c(1995,2)))

shapiro.test(window(rstandard(m1.co2), start=c(1995,2)))
```
Here we again see the one outlier in the upper tail, but the Shapiro-Wilk test of normality has a test statistic of $W=0.982$, leading to a p-value of $0.11$, and normality is not rejected at any of the usual significance levels.

### Auto-Correlation Residuals --- Box-Ljung Test
Box and Ljung or Box-Peerce proposed a test statistics defined as 
$$Q= n(n+2)\Bigg(\frac{\hat{r_1}^2}{n-1}+ \frac{\hat{r_2}^2}{n-2}+ \cdots+ \frac{\hat{r_k}^2}{n-k} \Bigg) \sim \chi_{k-p-q}^2 $$
```{r}
acf(residuals(m1.color), plot=F)$acf
signif(acf(residuals(m1.color), plot=F)$acf[1:6], 2)  # display the first 6 acf values to 2 significant digits
```
The Ljung-Box test statistic with K = 6 is equal to

$$Q= 35(35+2) \Bigg(\frac{(-0.051)^2}{35-1}+ \frac{(0.032)^2}{35-2}+ \frac{(0.047)^2}{35-3}+ \frac{(0.021)^2}{35-4}+ \frac{(-0.017)^2}{35-5}+ \frac{(-0.019)^2}{35-6}\Bigg) \approx 0.28$$
$$Q \approx 0.28 \sim \chi_{6-1-0}^2 \Rightarrow Q \approx 0.28 \sim \chi_{5}^2$$
Now compute the $p-value$
```{r}
p_value<- pchisq(0.28, df= 5, lower.tail= F)
p_value
```
The $p-value$ of $0.998$, so we have no evidence to reject the null hypothesis that the error terms are uncorrelated

**OR** Since p-value is not lesser than $0.05$ this implies that we don't reject $H_0$

```{r LBtest}
LB.test(m1.co2,lag=24)
```
The **Ljung-Box** test for this model gives a chi-squared value of $25.59$ with $22$ degrees of freedom, leading to a p-value of $0.27$—a further indication that the model has captured the dependence in the time series.

```{r}
tsdiag(m1.color, gof=15,omit.initial=F)
```

# FORECASTING
One of the primary objectives of building a model for a time series is to be able to forecast the values for that series at future times. Of equal importance is the assessment of the precision of those forecasts.

## Minimum Mean Square Error Forecasting
Based on the available history of the series up to time t, namely $Y_1, Y_2,..., Y_{t−1}, Y_t$, we would like to forecast the value of $Y_{t+l}$ that will occur $l$ time units into the future. We call time $t$ the forecast origin and $l$ the lead time for the forecast, and denote the forecast itself as $\hat{Y}_{t(l)}$.

The minimum mean square error forecast is given by
$$\hat{Y}_{t(l)}= E(Y_{t+l}| Y_1, Y_2,...,Y_t)$$

## Deterministic Trends
Consider once more the deterministic trend model
$$Y_t= \mu_t+ X_t$$
with $E(X_t)= 0$ and $V(X_t)= \gamma_0$ then, 
$$\hat{Y}_{t(l)}= E(\mu_{t+l}+ X_{t+l} | Y_1, Y_2,...,Y_t) $$
$$= E(\mu_{t+l}| Y_1, Y_2,...,Y_t) +  E(X_{t+l} | Y_1, Y_2,...,Y_t)$$
$$\hat{Y}_{t(l)}= \mu_{t+l}+  E(X_{t+l})$$
$$\hat{Y}_{t(l)}= \mu_{t+l}+ 0$$
or
$$\hat{Y}_{t(l)}= \mu_{t+l} \quad --- (1)$$
For the linear trend case, $\mu_t= \beta_0+ \beta_1 t$, the forecast is
$$\hat{Y}_{t(l)}= \beta_0+ \beta_1 (t+l)$$
For **seasonal model**, we have
$$\mu_t= \mu_{t+12}$$
Our forecast is
$$\hat{Y}_{t(l)}= \mu_{t+12+l}= \hat{Y}_{t(l+12)}=$$
Thus the forecast will also be periodic, as desired.

The **Forecast Error**, $\big(e_{t}(l)\big)$ is given by 
$$e_{t}(l)= Y_{t+l}- \hat{Y}_{t(l)}$$
$$= \mu_{t+l}+ X_{t+l}- \mu_{t+l}$$
$$e_{t}(l)= X_{t+l}$$
so that 
$$E(e_{t}(l))= E(X_{t+l})= 0$$
That is, the forecasts are **unbiased**. Also
$$Var(e_{t}(l))= Var(X_{t+l})= \gamma_0$$
is the forecast error variance for all lead times $l$.

**Example :** The cosine trend model for the average monthly temperature series was estimated as
$$\hat{\mu_t}= 46.26660+ (-2.1697) (2\pi t)+ (-2.1697)sin(2 \pi t)$$
Here time is measured in years with a starting value of January 1964, frequency $f=1$ per year and the final observed value is for $\text{December 1975}$. To forecast the $\text{June 1976}$ temperature value, we use $t=1976.41667$ as the time value and obtain
$$\hat{\mu_t}= 46.2660+ (-26.7079)cos(2\pi (1976.41667))+ (–2.1697)sin(2\pi (1976.41667))$$
$$\hat{\mu_t}= 68.3\,\, ^oF$$

## ARIMA Forecasting

### AR(1)
We shall first illustrate many of the ideas with the simple AR(1) Model
$$Y_t- \mu= \phi(Y_{t-1}- \mu)+ e_t \quad --- (2)$$
Consider the problem of forecasting one time unit into the future. Replacing $t$ by $t+1$ in the above expression, we have
$$Y_{t+1}- \mu= \phi(Y_t- \mu)+ e_{t+1} \quad --- (3)$$
Given $Y_1, Y_2,..., Y_{t−1}, Y_t$, we take the conditional expectations of both sides of Equation (3) and obtain
$$\hat{Y_t}(l)- \mu= \phi E\Big[E(Y_t| Y_1, Y_2,...,Y_t)- \mu\Big]+ E(e_{t+1| Y_1, Y_2,...,Y_t})$$
Now, from the properties of conditional expectation, we have conditional expectation of
$$E(Y_t| Y_1, Y_2,...,Y_t)= Y_t$$
Also, since $e_{t+l}$ is independent of $Y_1, Y_2,...,Y_{t-1}, Y_t$, we obtain
$$E(e_{t+1| Y_1, Y_2,...,Y_t})= E(e_{t+l})= 0$$
Thus,
$$\hat{Y_t}(l)= \phi(Y_t- \mu)+ \mu$$
Similarly, we consider a general lead time $l$
$$\hat{Y_t}(l)= \mu+ \phi(\hat{Y_{t}}(l-1)- \mu)$$
$$= \phi\big[\hat{Y_{t}}(l-1)- \mu \big]+ \mu$$
$$= \phi \big[\phi(\hat{Y_{t}}(l-2)+ \mu- \mu) \big]+ \mu$$
$$= \vdots$$
$$\hat{Y_t}(l)= \phi^l(Y_t- \mu)+ \mu$$

**Numerical Illustration :**
```{r}
data("color")

m1.color<- arima(color, order= c(1,0,0))
m1.color
```

Thus $\hat{\phi}= 0.5705$ and $\hat{\mu}= 74.3293$. The lost property observed is $67$. So,
$$\hat{Y_t}(1)= 74.3293+ (0.5705)(67- 74.3293)= 70.14793$$
For lead time 2
$$\hat{Y_t}(2)= 74.3293+ (0.5705)(70.14793- 74.3293)= 71.94383$$

Alternatively,
$$\hat{Y_t}(2)= 74.3293+ (0.5705)^2(67- 74.3293)= 71.92823$$

At lead 5
$$\hat{Y_t}(5)= 74.3293+ (0.5705)^5(67- 74.3293)= 73.88636$$

Similarly,
$$\hat{Y_t}(10)= 74.3293+ (0.5705)^{10}(67- 74.3293)= 74.30253$$
which is very nearly $\mu(=74.3293)$.

Thus, we have $|\phi|<1$, then we have
$$\hat{Y_t}(l) \approx \mu \quad \text{for large l}$$

# Seasonal Models
In earlier case we saw how seasonal deterministic trends might be modeled. However in many areas time series are used particularly businesses and economics. The assumption of any deterministic trend is guide suspect even though cyclical trend are very common in such series.

  Here is an example of labels of $CO_2$ are monitor at several sights around the world to investigate atmospheric changes. One of the sight is alert (North- West Territories Canada) near the architic circle.
```{r}
data("co2")
plot(co2, ylab= expression(CO[2]))

plot(window(co2, start=c(2000,1)), ylab= expression(CO[2]))
Month<- c('J','F','M','A','M','J','J','A','S','O','N','D')
points(window(co2,start=c(2000,1)),pch=Month)
```

## Seasonal ARIMA Models
We begin by studying stationary models and then consider nonstationary generalizations. We let $s$ denote the known seasonal period; for monthly series $s=12$ and for quarterly series $s=4$.

Consider the time series generated according to
$$Y_t= e_t- \Theta e_{t-12}$$
Notice that
$$Cov(Y_t, Y_{t-1})= Cov(e_t- \Theta e_{t-12}, \,\,e_{t-1}- \Theta e_{t-13})= 0 $$

but that
$$Cov(Y_t, Y_{t-12})= Cov(e_t- \Theta e_{t-12}, \,\,e_{t-12}- \Theta e_{t-24})$$

$$Cov(Y_t, Y_{t-12})= -\Theta \sigma_e^2$$
It is easy to see that such a series is stationary and has nonzero autocorrelations only at $lag\,\, 12$.

## Model Specification
As always, a careful inspection of the time series plot is the first step.
```{r}
plot(co2, ylab= expression(CO[2]))
```

The above display displays monthly carbon dioxide $CO_2$ levels in northern Canada. The upward trend alone would lead us to specify a nonstationary model.
```{r}
acf(as.vector(co2), lag.max=36)
```

The above plot shows sample auto-correlation function in that series. The seasonal auto-correlation relationships are
shown quite prominently in this display. 

Notice the strong correlation at $\text{lags 12, 24, 36, and so on}$. In addition, there is substantial other correlation that needs to be modeled.
```{r}
plot(diff(co2), ylab= 'First Difference of CO2', xlab= 'Time')
```

The general upward trend has now disappeared but the strong seasonality is still
present, as evidenced by the behavior shown in the above figure. Perhaps seasonal differencing will bring us to a series that may be modeled parsimoniously.
```{r}
acf(as.vector(diff(co2)), lag.max= 36)
```

From above plot it shows strong seasonality, which could be removed by plotting seasonal differences.
```{r}
plot(diff(diff(co2), lag= 12), xlab= 'Time', ylab= 'First and Seasonal Difference of CO2')

acf(as.vector(diff(diff(co2), lag=12)), lag.max=36, ci.type='ma')
```

confirms that very little autocorrelation remains in the series after these two differences have been taken. This plot also suggests that a simple model which incorporates the $lag\,\, 1$ and $lag\,\, 12$ autocorrelations might be adequate.

  We will consider specifying the multiplicative, seasonal $ARIMA(0,1,1) \times (0,1,1)_{12}$ model.

$$\nabla_{12} \nabla_{Y_t}= e_t- \theta e_{t-1}- \Theta e_{t-12}+ \theta \Theta e_{t-13}$$

### Model Fitting
```{r}
m1.co2<- arima(co2, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
m1.co2
```

# TIME SERIES MODELS OF HETEROSCEDASTICITY
The models discussed so far concern the conditional mean structure of time series data. However, more recently, there has been much work on modeling the conditional variance structure of time series data---mainly motivated by the needs for financial modeling. 

 Let ${Y_t}$ be a time series of interest. The conditional variance of $Y_t$ given the past $Y$ values, $Y_{t − 1},Y_{t − 2}, \dots$, measures the uncertainty in the deviation of $Y_t$  from its conditional mean $E(Y_t|Y_{t-1}, Y_{t-2}, \cdots)$. If $Y_t$, follows some ARIMA model, the (one-step ahead) conditional variance is always equal to the noise variance for any present and past values of the process. Indeed, the constancy of the conditional variance is true for predictions of any fixed number of steps ahead for an ARIMA process. In practice, the (one-step-ahead) conditional variance may vary with the current and past values of the process, and, as such, the conditional variance is itself a random process, often referred to as the conditional variance process. For example, daily returns of stocks are often observed to have larger conditional variance following a period of violent price movement than a relatively stable period. The development of models for the conditional variance process with which we can predict the variability of future values based on current and past data is the main concern of the present chapter.

## Some Common Features of Financial Time Series
As an example of financial time series, we consider the daily values of a unit of the CREF stock fund over the period from August 26, 2004 to August 15, 2006. The CREF stock fund is a fund of several thousand stocks and is not openly traded in the stock market.  Since stocks are not traded over weekends or on holidays, only on so-called trading days, the CREF data do not change over weekends and holidays. For simplicity, we will analyze the data as if they were equally spaced. Exhibit 12.1 shows the time series plot of the CREF data. It shows a generally increasing trend with a hint of higher variability with higher level of the stock value. Let ${p_t}$ be the time series of, say, the daily price of some financial asset. The (continuously compounded) return on the $t^{th}$ day is defined as
$$r_t= log(p_t)-log(p_{t-1})$$

Sometimes the returns are then multiplied by 100 so that they can be interpreted as percentage changes in the price. The multiplication may also reduce numerical errors as the raw returns could be very small numbers and render large rounding errors in some calculations.

```{r}
data(CREF)
plot(CREF)
```


```{r}
r.cref<- diff(log(CREF))*100
plot(r.cref, main="Daily CREF Stock Returns: August 26, 2004 to August 15,2006")
abline(h=0)

mean(r.cref)

sd(r.cref)/sqrt(length(r.cref))
```
Form above plot it is clear that returns were more volatile over some time periods and became very volatile toward the end of the study period. The average CREF return equals 0.0493 with a standard error of 0.02885.

```{r}
acf(r.cref,main="Sample ACF of Daily CREF Returns")
pacf(r.cref,main="Sample PACF of Daily CREF Returns")

acf(abs(r.cref),main="Sample ACF of the Absolute Daily CREF Returns")
pacf(abs(r.cref),main="Sample PACF of the Absolute Daily CREF Returns")

acf(r.cref^2,main="Sample ACF of the Squared Daily CREF Returns")
pacf(r.cref^2,main=" Sample PACF of the Squared Daily CREF Returns")
```

However, the volatility clustering observed in the CREF return data gives us a hint that they may not be independently and identically distributed—otherwise the variance would be constant over time. This is the first occasion in our study of time series models where we need to distinguish between series values being uncorrelated and series values being independent. If series values are truly independent, then nonlinear instantaneous transformations such as taking logarithms, absolute values, or squaring preserves independence. However, the same is not true of correlation, as correlation is only a measure of linear dependence. Higher-order serial dependence structure in data can be explored by studying the autocorrelation structure of the absolute returns (of lesser sampling variability with less mathematical tractability) or that of the squared returns (of greater sampling variability but with more manageability in terms of statistical theory). If the returns are independently and identically distributed, then so are the absolute returns (as are the squared returns), and hence they will be white noise as well. Hence, if the absolute or squared returns admit some significant autocorrelations, then these autocorrelations furnish some evidence against the hypothesis that the returns are independently and identically distributed. Indeed, the sample ACF and PACF of the absolute returns and those of the squared returns in Exhibits 12.5 through 12.8 display some significant autocorrelations and hence provide some evidence that the daily CREF returns are not independently and identically distributed.

## GARCH Models
The $ARCH(q)$ model, proposed by Engle (1982), by specifying that 
$$\sigma_{t|t-1}^2= \omega+ \alpha_1r_{t-1}^2+ \alpha_2r_{t-2}^2+ \cdots+ \alpha_qr_{t-q}^2$$

Here, $q$ is referred to as the $ARCH$ order. Another approach, proposed by Bollerslev (1986) and Taylor (1986), introduces $p$ lags of the conditional variance in the model, where $p$ is referred to as the $GARCH$ order. The combined model is called the generalized auto-regressive conditional heteroscedasticity, $GARCH(p,q)$, model.
$$\sigma_{t|t-1}^2= \omega+ \beta_1\sigma_{t-1|t-2}^2+ \cdots+ \beta_p\sigma_{t-p|t-p-1}^2+  \alpha_1r_{t-1}^2+ \alpha_2r_{t-2}^2+ \cdots+ \alpha_qr_{t-q}^2$$

In terms of the backshift $B$ notation, the model can be expressed as
$$(1- \beta_1B- \cdots- \beta_pB^2)\sigma_{t|t-1}^2= \omega+ (\alpha_1B+ \cdots+ \alpha_qB^q)r_t^2$$


Now  simulate data from a $GARCH(1, 1)$ model with standard normal innovations and parameter values $\omega= 0.02$, $\alpha= 0.05$, and $\beta= 0.9$. with model

$$\sigma_{t|t-1}^2= \omega+ \alpha r_{t-1}^2+ \beta \sigma_{t-1|t-2}^2$$

```{r, message=FALSE, warning=FALSE}
library(TSA)

set.seed(1234567)
garch11.sim<- garch.sim(alpha= c(0.02,0.05), beta= 0.9, n= 500)

plot(garch11.sim, type='l', ylab= expression(r[t]), xlab= 't', mai= "Simulated GARCH(1,1) Process")
```

Volatility clustering is evident in the plot, as large(small) fluctuations are usually succeeded by large(small) fluctuations.
```{r}
acf(garch11.sim)

pacf(garch11.sim)
```

Except for $lags\,3$ and $lag\,20$, which are mildly significant, the sample ACF and PACF of the simulated data, do not show significant correlations. Hence, the simulated process seems to be basically serially uncorrelated as it is.

Now we shall plot sample `ACF` and `PACF` of the absolute values and the squares of the simulated data.
```{r}
acf(abs(garch11.sim), main= "Sample ACF of the Absolute Values of GARCH(1,1) Process")

pacf(abs(garch11.sim), main= "Sample PACF of the Absolute Values of GARCH(1,1) Process")

acf(garch11.sim^2, main= "Sample ACF of the Squared Values of GARCH(1,1) Process")

pacf(garch11.sim^2, main= "Sample PACF of the Squared Values of GARCH(1,1) Process")
```

These plots indicate the existence of significant autocorrelation patterns in the absolute and squared data and indicate that the simulated process is in fact serially dependent. Interestingly, the $lag\,1$ auto-correlations are not significant in any of these last four plots.

## Maximum Likelihood Estimation

### GARCH(1, 1)
For $GARCH(1, 1)$ given the parameters $\omega, \alpha$ and $\beta$ the conditional variances can be computed by the formula

$$\sigma_{t|t-1}^2= \omega+ \alpha r_{t-1}^2+ \beta \sigma_{t-1|t-2}^2$$

for $t \ge2$ with likelihood
$$L(\omega, \alpha, \beta)= -\frac{n}{2}log(2\pi)- \frac{1}{2} \sum_{i=1}^n \Big\{log(\sigma_{t-1|t-2}^2)+ r_t^2/\sigma_{t|t-1}^2 \Big\} $$
We shall maximize this function with respect to $\omega, \alpha, \beta$ using the function $GARCH$ --- `garch()` of **tseries** package
```{r, message=FALSE, warning=FALSE}
library(tseries)

g1<- garch(garch11.sim, order= c(2,2))
summary(g1)
```

None of the above coefficients are significant. This is obvious because data was simulated from **GARCH(1, 1)** Model.

Now fit **GARCH(1,1)** Model

```{r}
g2<- garch(garch11.sim, order= c(1,1))
summary(g2)
```

Hence, $\alpha$ and $\beta$ are significant because data has been generated from **GARCH(1,1)**.

# Sequential Solution for Linear Gaussian State-Space Model
This chapter describe the sequential estimation method for the case where the observations are obtained sequentially in the linear gaussian state space model. This solution is called **Kalman Filter**.

## Kalman Filter
The optional sequential estimation method for the linear gaussian state space model is refered to as Kalman Filter named after Kalman. The word optional have means to minimize the MSE between point estimates and true values of the data to be estimated.

**Model :**
$$X_t= G_t x_{t-1}+ w_t \quad \therefore \{w_t \sim N(0, W_t)$$
$$y_t= F_tx_t+ v_t \therefore \{ v_t \sim N(0, V_t)$$
where 

* $G_t$ is $p\times p$ state transition matrix.
* $F_t$ is $1\times p$ observation matrix.
* $W_t$ is $p\times p$ covariance matrix of the state noise.
* $V_t$ is variance of the observation noise.

Regarding the prior distribution, we also assume that $x_0 \sim N(m_0, C_0)$ where $m_0$ is the $p$ dimensional mean vector and $C_0$ is the $p\times p$ covariance matrix.

Note that listing all the parameter in this linear gaussian state yield $\theta= \{G_t, F_t, W_t, V_t, m_0, C_0\}$ model.

### Kalman Filtering 
We define filtering distribution $N(m_t, C_t)$ for the mean vector $m_t$ and variance covariance matrix $C_t$.

One step ahead predictive distribution $N(a_t, R_t)$ for mean $a_t$ and variance covariance matrix $R_t$.

One step ahead predictive likelihood $N(f_t, Q_t)$ for mean $f_t$ and variance covariance matrix $Q_t$.

### Algorithm --- `Kalman Filtering` 
0. Filtering distribution at time point $t-1 : m_{t-1}, C_{t-1}$

1. Update procedure at time point $t$
  
  * One step ahead predictive distribution 
  
    * (Mean) $a_t \leftarrow G_t m_{t-1}$
    * (Covariave) $R_t \leftarrow G_t C_{t-1} G_t^\top+ W_t$

  * One step ahead predictive likelihood :
  
    * (Mean) $f_t \leftarrow F_t a_t$
    * (Covariance) $Q_t \leftarrow F_t R_t F_t^\top + V_t$

  * Kalman Gain 
  
    * $K_t \leftarrow R_t F_t^\top Q_t^{-1}$
    
  * State Update
  
    * (Mean) $m_t \leftarrow a_t+ K_t[y_t- f_t]$
    * (Covariance) $C_t= [I- K_t F_t]R_t$
  
2. Filtering distribution at time point $t : m_t, C_t$.


### Algorithm --- `Kalman Prediction`
It shows predictive distribution $\sim N(a_t(k), R_t(k))$ 

0. The $k-1$ step for obtaining the $k$ step ahead predictive distribution at time point $t+ (k-1): a_t(k-1), R_t(k-1)$
1. Updating procedure at time point $t+k$

  * the $k$ steps ahead predictive distribution
    
    * (Mean) $a_t(k) \leftarrow G_{t+k} a_t(k-1)$
    * (Covariance) $R_t(k) \leftarrow G_{t+k} R_t(k-1) G_{t+k}^\top+ W_{t+k}$
    
2. The $k$ step ahead predictive distribution at time point $t+k: a_t(k), R_t(k)$


### Algorithm --- `Kalman Smoothing`
Smoothing distribution $\sim N(s_t, S_t)$ for the mean vector $s_t$ and covariance matrix $S_t$

0. Smoothing distribution at time $t+1: s_{t+1}, S_{t+1}$
1. Update procedure at time $t$

  * Smoothing Gain $A_t \leftarrow C_t G_{t+1}^\top R_{t+1}^{-1}$
  * State Update
  
    * (Mean) $s_t \leftarrow m_t+ A_t[s_{t+1}- a_{t+1}]$
    * (Covariance) $S_t \leftarrow C_t+ A_t[S_{t+1}- R_{t+1}]A_t^\top$
    
2. Smoothing distribution at time $t: s_t, S_t$

## Example: Local-Level Model Case
The local-level model is defined as follows :
$$x_t= x_{t-1}+ w_t, \quad wt \sim N(0, W)$$
$$y_t= x_t+ v_t, \quad v_t \sim N(0, V)$$

The above model corresponds to the linear Gaussian state-space model that has setting $p= 1, G_t= [1], F_t= [1], W_t= W, V_t= V$

The local-level model is similar to the $AR(1)$ model. The difference lies in the $AR(1)$ coefficient $\phi$ for the state $x_{t-1}$, the coefficient of the local-level model is one. Such a coefficient of the non-stationary time series for the state known as a **random walk**. For this reason, the local-level model is also called a *random walk plus noise model*.  

**Define local-level model**
```{r, message=FALSE, warning=FALSE}
# Preprocessing
library(dlm)

# Setting of state space model
mod<- dlmModPoly(order= 1)

# Confirmation of model contents
str(mod)
```

### Specification of Parameter Values
IN this example, the parameters $W$ and $V$ are unknown, we must specify them. We attempt to use the maximum likelihood method for `*Nile*` data as it has sufficient data.

```{r}
# Specification of parameter values in local-level model

# User-defined function to define and build a model
build_dlm<- function(par){
  mod$W[1, 1]= exp(par[1])
  mod$V[1, 1]= exp(par[2])
  
  return(mod)
}

# Maximum Likelihood Estimation of parameters, confirming the search results with three different initial values
lapply(list(c(0,0), c(1, 10), c(20, 3)),
       function(parms){
         dlmMLE(y= Nile, parm= parms, build= build_dlm)
       })

# MLE of parameters, including the Hassian matrix in return value
fit_dlm<- dlmMLE(y= Nile, parm= c(0, 0), buil=  build_dlm, hessian= T)

# Find the a(symptotic) standard error in the MLE from the Hassian matrix using the delta method
exp(fit_dlm$par)*sqrt(diag(solve(fit_dlm$hessian)))

# Set the maximum likelihood estimates of parameters in the model
mod<- build_dlm(fit_dlm$par)

# Confirmation of the results
mod
```

$$
H= - 
 \begin{bmatrix}
     \frac{\partial^2l(W,V)}{\partial W \partial W} & \frac{\partial^2l(W,V)}{\partial W \partial V} \\
     \frac{\partial^2l(W,V)}{\partial V \partial W} & \frac{\partial^2l(W,V)}{\partial V \partial V}
 \end{bmatrix}
$$

We use the delta method to obtain the standard error of the parameters after exponential transformation.

The basic idea of the delta method method lies in Taylor expansions for the statistics of interest, these are simply treated at order.
Thus,
$$Var[exp(\theta)] \approx Var\Bigg[exp(\hat{\theta})+ \frac{exp'(\hat{\theta})}{1!}(\theta- \hat{\theta}) \Bigg] $$
$$exp^2(\hat{\theta})Var(\theta- \hat{\theta})$$
$$Var[exp(\theta)]= exp^2(\hat{\theta})Var(\theta)$$

Hence
$$exp(\text{maximum likelihood estimates of a parameter}) \times \sqrt{diag(solve(H))}$$

We have obtain SE of $W$ and $V$, repectively as $1280.170$ and $3145.999$.

### Execution of Filtering, Prediction and Smoothing
We examine the results through the plots with the $mean \pm 1.96\, se$.

**Filtering**

The operation of Kalman filtering is based on `Kalman Filtering` Algorithm. We perform the processing using library **dlm**
```{r}
# Filtering Process
dlmFiltered_obj<- dlmFilter(y= Nile, mod= mod)

# Confirmation of the results
str(dlmFiltered_obj, max.level= 1)

# Find the mean and SD of the filtering distribution
m<- dropFirst(dlmFiltered_obj$m)

m_sdev<- sqrt(
  dropFirst(as.numeric(
    dlmSvd2var(dlmFiltered_obj$U.C, dlmFiltered_obj$D.C)
  ))
)

# Find 2.5% and 97.5% values for 95% interval of the filtering distribution
m_quant<- list(m+qnorm(0.025, sd= m_sdev), 
               m+qnorm(0.975, sd= m_sdev))

# Plot results
ts.plot(cbind(Nile, m, do.call("cbind", m_quant)),
        col= c("lightgray", "black", "black", "black"),
        lty= c("solid", "solid", "dashed", "dashed"))

# Legend
legend(legend= c("Observations", "Mean (Filtering distribution)", "95% intervals (Filtering distribution)"),
       lty= c("solid", "solid", "dashed"),
       col= c("lightgray", "black", "black"),
       x= "topright", text.width= 32, cex= 0.6)
```

**Prediction**

The operation of Kalman prediction is based on `Kalman Prediction` Algorithm 
```{r}
# Prediction Processing
dlmForecast_obj<- dlmForecast(mod= dlmFiltered_obj, nAhead= 10)

# Confirmation of the results
str(dlmForecast_obj, max.level= 1)

# Find the mean and SD of the predictive distribution
a<- ts(data= dlmFiltered_obj$a, start= c(1971, 1))

a_sdev<- sqrt(
  as.numeric(
    dlmForecast_obj$R
  )
)

# Find 2.5% and 97.5% values for 95% interval of the predictive distribution
a_quant<- list(a+qnorm(0.025, sd= a_sdev), 
               a+qnorm(0.975, sd= a_sdev))

# Plot results
ts.plot(cbind(Nile, a, do.call("cbind", a_quant)),
        col= c("lightgray", "black", "black", "black"),
        lty= c("solid", "solid", "dashed", "dashed"))

# Legend
legend(legend= c("Observations", "Mean (Predictive distribution)", "95% intervals (Predictive distribution)"),
       lty= c("solid", "solid", "dashed"),
       col= c("lightgray", "black", "black"),
       x= "topright", text.width= 32, cex= 0.6) 
```

**Smoothing**
The operation of Kalman prediction is based on `Kalman Smoothing` Algorithm
```{r}
# Smoothing Processing
dlmSmoothed_obj<- dlmSmooth(y= Nile, mod= mod)

# Confirmation of the results
str(dlmSmoothed_obj, max.level= 1)

# Find the mean and SD of the predictive distribution
s<- dropFirst(dlmSmoothed_obj$s)

s_sdev<- sqrt(
  dropFirst(as.numeric(
    dlmSvd2var(dlmSmoothed_obj$U.S, dlmSmoothed_obj$D.S)
  ))
)

# Find 2.5% and 97.5% values for 95% interval of the smoothing distribution
s_quant<- list(s+qnorm(0.025, sd= s_sdev), 
               s+qnorm(0.975, sd= s_sdev))

# Plot results
ts.plot(cbind(Nile, s, do.call("cbind", s_quant)),
        col= c("lightgray", "black", "black", "black"),
        lty= c("solid", "solid", "dashed", "dashed"))

# Legend
legend(legend= c("Observations", "Mean (Smoothing distribution)", "95% intervals (Smoothing distribution)"),
       lty= c("solid", "solid", "dashed"),
       col= c("lightgray", "black", "black"),
       x= "topright", text.width= 32, cex= 0.6) 
```

### Diagnostic Checking Results
Finally, we explain diagnostic checking for the results. We use the likelihood and prediction error as diagnostic tools.

**Likelihood**

```{r}
# Likelihood in Linear Gaussian State space model
# Calculation of "negative" log-likelihood
dlmLL(y= Nile, mod= mod)
```

**Innovation (Prediction Error / Residuals)**

The difference between observations and the mean of the one-step-ahead predictive likelihood, i.e., $e_t= y_t- f_t$, is called *Innovation, Prediction Error* or *Residuals*. If the model is appropriate, the innovations theoretically follow an independent and identical normal distribution. Note that $e_t/\sqrt{Q_t}$ becomes *normalized innovations*.
```{r}
# < Model Diagnosis Using Innovations >
# Adjust display area
oldpar<- par(no.readonly= T)
par(oma= c(0,0,0,0))
par(mar= c(4,4,3,1))

# Confirmation of Auto-correlation
tsdiag(object= dlmFiltered_obj)
par(oldpar)

# Confirm Normality
# Get standardized innovation
e<- residuals(object= dlmFiltered_obj, sd= F)
e

# Display Q-Q Plot
qqnorm(e)
qqline(e)
```


```
require(rmarkdown)
render("tsaw.qmd", word_document(toc = T, number_sections = T))
```

